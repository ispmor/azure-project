{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Machine Learning Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from dotenv import set_key, get_key, find_dotenv\n",
    "from pathlib import Path\n",
    "from utilities import get_auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are currently using version 1.17.0 of the Azure ML SDK\n"
     ]
    }
   ],
   "source": [
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = \"1db0a5ce-7de1-4082-8e25-3c5a4e5a9a98\"\n",
    "resource_group = \"ProjektAzure\"\n",
    "workspace_name = \"ProjektAzure\"\n",
    "workspace_region = \"East US\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = find_dotenv()\n",
    "if env_path == \"\":\n",
    "    Path(\".env\").touch()\n",
    "    env_path = find_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'workspace_region', 'East US')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_key(env_path, \"subscription_id\", subscription_id) \n",
    "set_key(env_path, \"resource_group\", resource_group)\n",
    "set_key(env_path, \"workspace_name\", workspace_name)\n",
    "set_key(env_path, \"workspace_region\", workspace_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the workspace using the specified parameters\n",
    "ws = Workspace.create(\n",
    "    name=workspace_name,\n",
    "    subscription_id=subscription_id,\n",
    "    resource_group=resource_group,\n",
    "    location=workspace_region,\n",
    "    create_resource_group=True,\n",
    "    auth=get_auth(env_path),\n",
    "    exist_ok=True,\n",
    ")\n",
    "\n",
    "# write the details of the workspace to a configuration file\n",
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '/subscriptions/1db0a5ce-7de1-4082-8e25-3c5a4e5a9a98/resourceGroups/ProjektAzure/providers/Microsoft.MachineLearningServices/workspaces/ProjektAzure',\n",
       " 'name': 'ProjektAzure',\n",
       " 'location': 'eastus',\n",
       " 'type': 'Microsoft.MachineLearningServices/workspaces',\n",
       " 'sku': 'Basic',\n",
       " 'workspaceid': 'ec3d9bcd-0a08-464f-ae43-b69afde74018',\n",
       " 'sdkTelemetryAppInsightsKey': 'f5784ccd-178d-4ecc-9998-b05841b44ae9',\n",
       " 'description': '',\n",
       " 'friendlyName': 'ProjektAzure',\n",
       " 'creationTime': '2020-12-21T20:48:27.3350460+00:00',\n",
       " 'keyVault': '/subscriptions/1db0a5ce-7de1-4082-8e25-3c5a4e5a9a98/resourcegroups/projektazure/providers/microsoft.keyvault/vaults/projektakeyvaultceb809a5',\n",
       " 'applicationInsights': '/subscriptions/1db0a5ce-7de1-4082-8e25-3c5a4e5a9a98/resourcegroups/projektazure/providers/microsoft.insights/components/projektainsights99765023',\n",
       " 'identityPrincipalId': '6bcd4c16-2e88-4cd9-afbd-81b67e195a4c',\n",
       " 'identityTenantId': '3b50229c-cd78-4588-9bcf-97b7629e2f0f',\n",
       " 'identityType': 'SystemAssigned',\n",
       " 'storageAccount': '/subscriptions/1db0a5ce-7de1-4082-8e25-3c5a4e5a9a98/resourcegroups/projektazure/providers/microsoft.storage/storageaccounts/projektastorage1d32b8c8a',\n",
       " 'hbiWorkspace': False,\n",
       " 'discoveryUrl': 'https://eastus.experiments.azureml.net/discovery',\n",
       " 'notebookInfo': {'fqdn': 'ml-projektazure-eastus-ec3d9bcd-0a08-464f-ae43-b69afde74018.notebooks.azure.net',\n",
       "  'resource_id': '9b6128a19f3147b8aef03640273f1951'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load workspace configuration\n",
    "ws = Workspace.from_config(auth=get_auth(env_path))\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/XMLDataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/XMLDataset.py\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class BuildDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"Data/JPEGImages\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"Data/JPEGImages\", self.imgs[idx])\n",
    "        xml_path = os.path.join(\n",
    "            self.root, \"Data/Annotations\", \"{}.xml\".format(self.imgs[idx].strip(\".jpg\"))\n",
    "        )\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # parse XML annotation\n",
    "        tree = ET.parse(xml_path)\n",
    "        t_root = tree.getroot()\n",
    "\n",
    "        # get bounding box coordinates\n",
    "        boxes = []\n",
    "        for obj in t_root.findall(\"object\"):\n",
    "            bnd_box = obj.find(\"bndbox\")\n",
    "            xmin = float(bnd_box.find(\"xmin\").text)\n",
    "            xmax = float(bnd_box.find(\"xmax\").text)\n",
    "            ymin = float(bnd_box.find(\"ymin\").text)\n",
    "            ymax = float(bnd_box.find(\"ymax\").text)\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        num_objs = len(boxes)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "\n",
    "        # area of the bounding box, used during evaluation with the COCO metric for small, medium and large boxes\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/maskrcnn_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/maskrcnn_model.py\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.rpn import RPNHead\n",
    "\n",
    "\n",
    "def get_model(\n",
    "    num_classes,\n",
    "    anchor_sizes,\n",
    "    anchor_aspect_ratios,\n",
    "    rpn_nms_threshold,\n",
    "    box_nms_threshold,\n",
    "    box_score_threshold,\n",
    "    num_box_detections,\n",
    "):\n",
    "\n",
    "    # load pre-trained mask R-CNN model\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n",
    "        pretrained=True,\n",
    "        rpn_nms_thresh=rpn_nms_threshold,\n",
    "        box_nms_thresh=box_nms_threshold,\n",
    "        box_score_thresh=box_score_threshold,\n",
    "        box_detections_per_img=num_box_detections,\n",
    "    )\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    anchor_sizes = tuple([float(i) for i in anchor_sizes.split(\",\")])\n",
    "    anchor_aspect_ratios = tuple([float(i) for i in anchor_aspect_ratios.split(\",\")])\n",
    "\n",
    "    # create an anchor_generator for the FPN which by default has 5 outputs\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=tuple([anchor_sizes for _ in range(5)]),\n",
    "        aspect_ratios=tuple([anchor_aspect_ratios for _ in range(5)]),\n",
    "    )\n",
    "    model.rpn.anchor_generator = anchor_generator\n",
    "\n",
    "    # get number of input features for the RPN returned by FPN (256)\n",
    "    in_channels = model.backbone.out_channels\n",
    "\n",
    "    # replace the RPN head\n",
    "    model.rpn.head = RPNHead(\n",
    "        in_channels, anchor_generator.num_anchors_per_location()[0]\n",
    "    )\n",
    "\n",
    "    # turn off masks since dataset only has bounding boxes\n",
    "    model.roi_heads.mask_roi_pool = None\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/train.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"./cocoapi/PythonAPI/\")\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import utils\n",
    "from XMLDataset import BuildDataset, get_transform\n",
    "from maskrcnn_model import get_model\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch Object Detection Training\")\n",
    "    parser.add_argument(\n",
    "        \"--data_path\", default=\"./Data/\", help=\"the path to the dataset\"\n",
    "    )\n",
    "    parser.add_argument(\"--batch_size\", default=2, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--epochs\", default=10, type=int, help=\"number of total epochs to run\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--workers\", default=4, type=int, help=\"number of data loading workers\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\", default=0.005, type=float, help=\"initial learning rate\"\n",
    "    )\n",
    "    parser.add_argument(\"--momentum\", default=0.9, type=float, help=\"momentum\")\n",
    "    parser.add_argument(\n",
    "        \"--weight_decay\",\n",
    "        default=0.0005,\n",
    "        type=float,\n",
    "        help=\"weight decay (default: 1e-4)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_step_size\", default=3, type=int, help=\"decrease lr every step-size epochs\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_gamma\",\n",
    "        default=0.1,\n",
    "        type=float,\n",
    "        help=\"decrease lr by a factor of lr-gamma\",\n",
    "    )\n",
    "    parser.add_argument(\"--print_freq\", default=10, type=int, help=\"print frequency\")\n",
    "    parser.add_argument(\"--output_dir\", default=\"outputs\", help=\"path where to save\")\n",
    "    parser.add_argument(\"--anchor_sizes\", default=\"16\", type=str, help=\"anchor sizes\")\n",
    "    parser.add_argument(\n",
    "        \"--anchor_aspect_ratios\", default=\"1.0\", type=str, help=\"anchor aspect ratios\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--rpn_nms_thresh\",\n",
    "        default=0.7,\n",
    "        type=float,\n",
    "        help=\"NMS threshold used for postprocessing the RPN proposals\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--box_nms_thresh\",\n",
    "        default=0.5,\n",
    "        type=float,\n",
    "        help=\"NMS threshold for the prediction head. Used during inference\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--box_score_thresh\",\n",
    "        default=0.05,\n",
    "        type=float,\n",
    "        help=\"during inference only return proposals\"\n",
    "        \"with a classification score greater than box_score_thresh\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--box_detections_per_img\",\n",
    "        default=100,\n",
    "        type=int,\n",
    "        help=\"maximum number of detections per image, for all classes\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "data_path = args.data_path\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset = BuildDataset(data_path, get_transform(train=True))\n",
    "dataset_test = BuildDataset(data_path, get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-100])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-100:])\n",
    "\n",
    "batch_size = args.batch_size\n",
    "workers = args.workers\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    collate_fn=utils.collate_fn,\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=workers,\n",
    "    collate_fn=utils.collate_fn,\n",
    ")\n",
    "\n",
    "\n",
    "# our dataset has two classes only - background and out of stock\n",
    "num_classes = 2\n",
    "\n",
    "model = get_model(\n",
    "    num_classes,\n",
    "    args.anchor_sizes,\n",
    "    args.anchor_aspect_ratios,\n",
    "    args.rpn_nms_thresh,\n",
    "    args.box_nms_thresh,\n",
    "    args.box_score_thresh,\n",
    "    args.box_detections_per_img,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = args.learning_rate\n",
    "momentum = args.momentum\n",
    "weight_decay = args.weight_decay\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params, lr=learning_rate, momentum=momentum, weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "lr_step_size = args.lr_step_size\n",
    "lr_gamma = args.lr_gamma\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=lr_step_size, gamma=lr_gamma\n",
    ")\n",
    "\n",
    "# number of training epochs\n",
    "num_epochs = args.epochs\n",
    "print_freq = args.print_freq\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=print_freq)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset after every epoch\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "# save model\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "torch.save(model.state_dict(), os.path.join(args.output_dir, \"model_latest.pth\"))\n",
    "\n",
    "print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TERENOWANIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.17.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"scripts\")\n",
    "sys.path.append(\"scripts/cocoapi/PythonAPI/\")\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.dnn import PyTorch\n",
    "\n",
    "from dotenv import set_key, get_key, find_dotenv\n",
    "from utilities import get_auth, download_data\n",
    "\n",
    "import torch\n",
    "from scripts.XMLDataset import BuildDataset, get_transform\n",
    "from scripts.maskrcnn_model import get_model\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = find_dotenv(raise_error_if_not_found=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Finished extracting.\n"
     ]
    }
   ],
   "source": [
    "data_file = \"Data.zip\"\n",
    "data_url = (\"https://bostondata.blob.core.windows.net/builddata/{}\".format(data_file))\n",
    "download_data(data_file, data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProjektAzure\n",
      "ProjektAzure\n",
      "eastus\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config(auth=get_auth(env_path))\n",
    "print(ws.name, ws.resource_group, ws.location, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(workspace=ws, name='torchvision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import sys\n",
      "\n",
      "sys.path.append(\"./cocoapi/PythonAPI/\")\n",
      "\n",
      "import torch\n",
      "import argparse\n",
      "import utils\n",
      "from XMLDataset import BuildDataset, get_transform\n",
      "from maskrcnn_model import get_model\n",
      "from engine import train_one_epoch, evaluate\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    parser = argparse.ArgumentParser(description=\"PyTorch Object Detection Training\")\n",
      "    parser.add_argument(\n",
      "        \"--data_path\", default=\"./Data/\", help=\"the path to the dataset\"\n",
      "    )\n",
      "    parser.add_argument(\"--batch_size\", default=2, type=int)\n",
      "    parser.add_argument(\n",
      "        \"--epochs\", default=10, type=int, help=\"number of total epochs to run\"\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--workers\", default=4, type=int, help=\"number of data loading workers\"\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--learning_rate\", default=0.005, type=float, help=\"initial learning rate\"\n",
      "    )\n",
      "    parser.add_argument(\"--momentum\", default=0.9, type=float, help=\"momentum\")\n",
      "    parser.add_argument(\n",
      "        \"--weight_decay\",\n",
      "        default=0.0005,\n",
      "        type=float,\n",
      "        help=\"weight decay (default: 1e-4)\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--lr_step_size\", default=3, type=int, help=\"decrease lr every step-size epochs\"\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--lr_gamma\",\n",
      "        default=0.1,\n",
      "        type=float,\n",
      "        help=\"decrease lr by a factor of lr-gamma\",\n",
      "    )\n",
      "    parser.add_argument(\"--print_freq\", default=10, type=int, help=\"print frequency\")\n",
      "    parser.add_argument(\"--output_dir\", default=\"outputs\", help=\"path where to save\")\n",
      "    parser.add_argument(\"--anchor_sizes\", default=\"16\", type=str, help=\"anchor sizes\")\n",
      "    parser.add_argument(\n",
      "        \"--anchor_aspect_ratios\", default=\"1.0\", type=str, help=\"anchor aspect ratios\"\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--rpn_nms_thresh\",\n",
      "        default=0.7,\n",
      "        type=float,\n",
      "        help=\"NMS threshold used for postprocessing the RPN proposals\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--box_nms_thresh\",\n",
      "        default=0.5,\n",
      "        type=float,\n",
      "        help=\"NMS threshold for the prediction head. Used during inference\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--box_score_thresh\",\n",
      "        default=0.05,\n",
      "        type=float,\n",
      "        help=\"during inference only return proposals\"\n",
      "        \"with a classification score greater than box_score_thresh\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--box_detections_per_img\",\n",
      "        default=100,\n",
      "        type=int,\n",
      "        help=\"maximum number of detections per image, for all classes\",\n",
      "    )\n",
      "    args = parser.parse_args()\n",
      "    \n",
      "data_path = args.data_path\n",
      "\n",
      "# use our dataset and defined transformations\n",
      "dataset = BuildDataset(data_path, get_transform(train=True))\n",
      "dataset_test = BuildDataset(data_path, get_transform(train=False))\n",
      "\n",
      "# split the dataset in train and test set\n",
      "indices = torch.randperm(len(dataset)).tolist()\n",
      "dataset = torch.utils.data.Subset(dataset, indices[:-100])\n",
      "dataset_test = torch.utils.data.Subset(dataset_test, indices[-100:])\n",
      "\n",
      "batch_size = args.batch_size\n",
      "workers = args.workers\n",
      "\n",
      "# define training and validation data loaders\n",
      "data_loader = torch.utils.data.DataLoader(\n",
      "    dataset,\n",
      "    batch_size=2,\n",
      "    shuffle=True,\n",
      "    num_workers=workers,\n",
      "    collate_fn=utils.collate_fn,\n",
      ")\n",
      "\n",
      "data_loader_test = torch.utils.data.DataLoader(\n",
      "    dataset_test,\n",
      "    batch_size=2,\n",
      "    shuffle=False,\n",
      "    num_workers=workers,\n",
      "    collate_fn=utils.collate_fn,\n",
      ")\n",
      "\n",
      "\n",
      "# our dataset has two classes only - background and out of stock\n",
      "num_classes = 2\n",
      "\n",
      "model = get_model(\n",
      "    num_classes,\n",
      "    args.anchor_sizes,\n",
      "    args.anchor_aspect_ratios,\n",
      "    args.rpn_nms_thresh,\n",
      "    args.box_nms_thresh,\n",
      "    args.box_score_thresh,\n",
      "    args.box_detections_per_img,\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# train on the GPU or on the CPU, if a GPU is not available\n",
      "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
      "\n",
      "# move model to the right device\n",
      "model.to(device)\n",
      "\n",
      "learning_rate = args.learning_rate\n",
      "momentum = args.momentum\n",
      "weight_decay = args.weight_decay\n",
      "\n",
      "# construct an optimizer\n",
      "params = [p for p in model.parameters() if p.requires_grad]\n",
      "optimizer = torch.optim.SGD(\n",
      "    params, lr=learning_rate, momentum=momentum, weight_decay=weight_decay\n",
      ")\n",
      "\n",
      "lr_step_size = args.lr_step_size\n",
      "lr_gamma = args.lr_gamma\n",
      "\n",
      "# and a learning rate scheduler\n",
      "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
      "    optimizer, step_size=lr_step_size, gamma=lr_gamma\n",
      ")\n",
      "\n",
      "# number of training epochs\n",
      "num_epochs = args.epochs\n",
      "print_freq = args.print_freq\n",
      "\n",
      "for epoch in range(num_epochs):\n",
      "    # train for one epoch, printing every 10 iterations\n",
      "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=print_freq)\n",
      "    # update the learning rate\n",
      "    lr_scheduler.step()\n",
      "    # evaluate on the test dataset after every epoch\n",
      "    evaluate(model, data_loader_test, device=device)\n",
      "\n",
      "# save model\n",
      "if not os.path.exists(args.output_dir):\n",
      "    os.makedirs(args.output_dir)\n",
      "torch.save(model.state_dict(), os.path.join(args.output_dir, \"model_latest.pth\"))\n",
      "\n",
      "print(\"That's it!\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"scripts/train.py\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 'use_docker' parameter will be deprecated. Please use 'environment_definition' instead.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "script_params = {\n",
    "    \"--data_path\": \".\",\n",
    "    \"--workers\": 8,\n",
    "    \"--learning_rate\": 0.005,\n",
    "    \"--epochs\": num_epochs,\n",
    "    \"--anchor_sizes\": \"16,32,64,128,256,512\",\n",
    "    \"--anchor_aspect_ratios\": \"0.25,0.5,1.0,2.0\",\n",
    "    \"--rpn_nms_thresh\": 0.5,\n",
    "    \"--box_nms_thresh\": 0.3,\n",
    "    \"--box_score_thresh\": 0.10,\n",
    "}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    source_directory=\"./scripts\",\n",
    "    script_params=script_params,\n",
    "    compute_target=\"local\",\n",
    "    entry_script=\"train.py\",\n",
    "    use_docker=False,\n",
    "    user_managed=True,\n",
    "    use_gpu=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.run_config.environment.python.interpreter_path = (\"/anaconda/envs/azureml_py36_pytorch/bin/python\")\n",
    "estimator.run_config.history.snapshot_project = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - If 'script' has been provided here and a script file name has been specified in 'run_config', 'script' provided in ScriptRunConfig initialization will take precedence.\n",
      "WARNING - If 'arguments' has been provided here and arguments have been specified in 'run_config', 'arguments' provided in ScriptRunConfig initialization will take precedence.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3e3c4225bc4c4790127073fbf3051f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/torchvision/runs/torchvision_1608660691_f488dc74?wsid=/subscriptions/1db0a5ce-7de1-4082-8e25-3c5a4e5a9a98/resourcegroups/ProjektAzure/workspaces/ProjektAzure\", \"run_id\": \"torchvision_1608660691_f488dc74\", \"run_properties\": {\"run_id\": \"torchvision_1608660691_f488dc74\", \"created_utc\": \"2020-12-22T18:11:33.498983Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"local\", \"ContentSnapshotId\": null, \"azureml.git.repository_uri\": \"https://github.com/ispmor/azure-project.git\", \"mlflow.source.git.repoURL\": \"https://github.com/ispmor/azure-project.git\", \"azureml.git.branch\": \"main\", \"mlflow.source.git.branch\": \"main\", \"azureml.git.commit\": \"0cd15c57f57b5894818909867bd32604aa9d5ad1\", \"mlflow.source.git.commit\": \"0cd15c57f57b5894818909867bd32604aa9d5ad1\", \"azureml.git.dirty\": \"True\"}, \"tags\": {\"mlflow.source.type\": \"JOB\", \"mlflow.source.name\": \"train.py\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2020-12-22T18:20:54.064672Z\", \"status\": \"Completed\", \"log_files\": {\"azureml-logs/60_control_log.txt\": \"https://projektastorage1d32b8c8a.blob.core.windows.net/azureml/ExperimentRun/dcid.torchvision_1608660691_f488dc74/azureml-logs/60_control_log.txt?sv=2019-02-02&sr=b&sig=yUC3nXNiGcTHMceBEPqEqPTtEqzstnkjJsp%2F2g2b7WQ%3D&st=2020-12-22T18%3A10%3A56Z&se=2020-12-23T02%3A20%3A56Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://projektastorage1d32b8c8a.blob.core.windows.net/azureml/ExperimentRun/dcid.torchvision_1608660691_f488dc74/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=H7tkw8B3qRgU0AsquXfPeNZEnqQHPay0tHayWLjugVM%3D&st=2020-12-22T18%3A10%3A56Z&se=2020-12-23T02%3A20%3A56Z&sp=r\", \"logs/azureml/100222_azureml.log\": \"https://projektastorage1d32b8c8a.blob.core.windows.net/azureml/ExperimentRun/dcid.torchvision_1608660691_f488dc74/logs/azureml/100222_azureml.log?sv=2019-02-02&sr=b&sig=ySjguslG0Ix783tZdllx1RLpQQwlehkR8iKuM565yBA%3D&st=2020-12-22T18%3A01%3A45Z&se=2020-12-23T02%3A11%3A45Z&sp=r\", \"logs/azureml/dataprep/python_span_b374fb72-2471-4b1e-ba7e-acce02202b4c.jsonl\": \"https://projektastorage1d32b8c8a.blob.core.windows.net/azureml/ExperimentRun/dcid.torchvision_1608660691_f488dc74/logs/azureml/dataprep/python_span_b374fb72-2471-4b1e-ba7e-acce02202b4c.jsonl?sv=2019-02-02&sr=b&sig=NASDltquOK7yHjFdesxBD%2BFYbSjj7PBqIqBsp6UwBTo%3D&st=2020-12-22T18%3A01%3A47Z&se=2020-12-23T02%3A11%3A47Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/60_control_log.txt\"], [\"azureml-logs/70_driver_log.txt\"], [\"logs/azureml/dataprep/python_span_b374fb72-2471-4b1e-ba7e-acce02202b4c.jsonl\"], [\"logs/azureml/100222_azureml.log\"]], \"run_duration\": \"0:09:20\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [{\"name\": \"mAP@IoU=0.50\", \"run_id\": \"torchvision_1608660691_f488dc74\", \"categories\": [0], \"series\": [{\"data\": [0.977188553048049]}]}], \"run_logs\": \"2020-12-22 18:11:40,470|azureml|DEBUG|Inputs:: kwargs: {'OutputCollection': True, 'EnableMLflowTracking': True, 'snapshotProject': False}, track_folders: None, deny_list: None, directories_to_watch: ['logs', 'logs/azureml']\\n2020-12-22 18:11:40,470|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Execution target type: none\\n2020-12-22 18:11:40,523|azureml.history._tracking.PythonWorkingDirectory|DEBUG|PySpark found in environment.\\n2020-12-22 18:11:40,523|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Pinning working directory for filesystems: ['pyfs']\\n2020-12-22 18:11:40,855|azureml.core._experiment_method|DEBUG|Trying to register submit_function search, on method <class 'azureml.train.hyperdrive.runconfig.HyperDriveRunConfig'>\\n2020-12-22 18:11:40,855|azureml.core._experiment_method|DEBUG|Registered submit_function search, on method <class 'azureml.train.hyperdrive.runconfig.HyperDriveRunConfig'>\\n2020-12-22 18:11:40,855|azureml.core._experiment_method|DEBUG|Trying to register submit_function search, on method <class 'azureml.train.hyperdrive.runconfig.HyperDriveConfig'>\\n2020-12-22 18:11:40,855|azureml.core._experiment_method|DEBUG|Registered submit_function search, on method <class 'azureml.train.hyperdrive.runconfig.HyperDriveConfig'>\\n2020-12-22 18:11:40,855|azureml.core.run|DEBUG|Adding new factory <function HyperDriveRun._from_run_dto at 0x7fd54309d510> for run source hyperdrive\\n2020-12-22 18:11:41,688|azureml.core.run|DEBUG|Adding new factory <function AutoMLRun._from_run_dto at 0x7fd530de68c8> for run source automl\\n2020-12-22 18:11:41,699|azureml.core.run|DEBUG|Adding new factory <function PipelineRun._from_dto at 0x7fd542dabd08> for run source azureml.PipelineRun\\n2020-12-22 18:11:41,708|azureml.core.run|DEBUG|Adding new factory <function StepRun._from_reused_dto at 0x7fd542d387b8> for run source azureml.ReusedStepRun\\n2020-12-22 18:11:41,717|azureml.core.run|DEBUG|Adding new factory <function StepRun._from_dto at 0x7fd542d38730> for run source azureml.StepRun\\n2020-12-22 18:11:41,726|azureml.core.run|DEBUG|Adding new factory <function ScriptRun._from_run_dto at 0x7fd543221e18> for run source azureml.scriptrun\\n2020-12-22 18:11:41,757|azureml.core.authentication.TokenRefresherDaemon|DEBUG|Starting daemon and triggering first instance\\n2020-12-22 18:11:41,764|azureml._restclient.clientbase|INFO|Created a worker pool for first use\\n2020-12-22 18:11:41,764|azureml.core.authentication|DEBUG|Time to expire 1814391.235377 seconds\\n2020-12-22 18:11:41,764|azureml._restclient.service_context|DEBUG|Created a static thread pool for ServiceContext class\\n2020-12-22 18:11:41,764|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 18:11:41,765|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 18:11:41,765|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 18:11:41,765|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 18:11:41,765|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 18:11:41,765|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 18:11:41,766|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 18:11:41,795|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[START]\\n2020-12-22 18:11:41,795|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.RunClient|DEBUG|ClientBase: Calling get_by_exp_id with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experimentids/{experimentId}/runs/{runId}\\n2020-12-22 18:11:41,861|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[STOP]\\n2020-12-22 18:11:41,862|azureml._SubmittedRun#torchvision_1608660691_f488dc74|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'local', 'ContentSnapshotId': None, 'azureml.git.repository_uri': 'https://github.com/ispmor/azure-project.git', 'mlflow.source.git.repoURL': 'https://github.com/ispmor/azure-project.git', 'azureml.git.branch': 'main', 'mlflow.source.git.branch': 'main', 'azureml.git.commit': '0cd15c57f57b5894818909867bd32604aa9d5ad1', 'mlflow.source.git.commit': '0cd15c57f57b5894818909867bd32604aa9d5ad1', 'azureml.git.dirty': 'True'}\\n2020-12-22 18:11:41,862|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2020-12-22 18:11:42,365|azureml|DEBUG|Installed with mlflow version 1.11.0.\\n2020-12-22 18:11:42,365|azureml.mlflow|DEBUG|Setting up a Remote MLflow run\\n2020-12-22 18:11:42,368|azureml.mlflow|DEBUG|Creating a tracking uri in eastus.experiments.azureml.net for workspace /subscriptions/1db0a5ce-7de1-4082-8e25-3c5a4e5a9a98/resourceGroups/ProjektAzure/providers/Microsoft.MachineLearningServices/workspaces/ProjektAzure\\n2020-12-22 18:11:42,368|azureml.mlflow._internal.store|DEBUG|Initializing the AzureMLRestStore\\n2020-12-22 18:11:42,368|azureml.mlflow._internal.model_registry|DEBUG|Initializing the AzureMLflowModelRegistry\\n2020-12-22 18:11:42,368|azureml.mlflow|DEBUG|Setting MLflow tracking uri env var\\n2020-12-22 18:11:42,368|azureml.mlflow|DEBUG|Setting MLflow run id env var with torchvision_1608660691_f488dc74\\n2020-12-22 18:11:42,368|azureml.mlflow|DEBUG|Setting Mlflow experiment with torchvision\\n2020-12-22 18:11:42,369|azureml.mlflow|DEBUG|Setting the mlflow tag mlflow.source.type\\n2020-12-22 18:11:42,370|azureml.mlflow|DEBUG|Setting the mlflow tag mlflow.source.name\\n2020-12-22 18:11:42,370|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.RunClient.get_details-async:False|DEBUG|[START]\\n2020-12-22 18:11:42,370|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.RunClient|DEBUG|ClientBase: Calling get_details with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experiments/{experimentName}/runs/{runId}/details\\n2020-12-22 18:11:42,615|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.RunClient.get_details-async:False|DEBUG|[STOP]\\n2020-12-22 18:11:42,617|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.RunClient.patch_by_exp_id-async:False|DEBUG|[START]\\n2020-12-22 18:11:42,617|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.RunClient|DEBUG|ClientBase: Calling patch_by_exp_id with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experimentids/{experimentId}/runs/{runId}\\n2020-12-22 18:11:42,727|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.RunClient.patch_by_exp_id-async:False|DEBUG|[STOP]\\n2020-12-22 18:11:42,727|azureml.WorkerPool|DEBUG|[START]\\n2020-12-22 18:11:42,727|azureml.SendRunKillSignal|DEBUG|[START]\\n2020-12-22 18:11:42,727|azureml.RunStatusContext|DEBUG|[START]\\n2020-12-22 18:11:42,727|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunContextManager.RunStatusContext|DEBUG|[START]\\n2020-12-22 18:11:42,727|azureml.MetricsClient|DEBUG|[START]\\n2020-12-22 18:11:42,727|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.MetricsClient|DEBUG|[START]\\n2020-12-22 18:11:42,727|azureml.ContentUploader|DEBUG|[START]\\n2020-12-22 18:11:42,728|azureml._history.utils.context_managers|DEBUG|starting file watcher\\n2020-12-22 18:11:42,729|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|[Start]\\n2020-12-22 18:11:42,729|azureml.TrackFolders|DEBUG|[START]\\n2020-12-22 18:11:42,729|azureml.WorkingDirectoryCM|DEBUG|[START]\\n2020-12-22 18:11:42,729|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[START]\\n2020-12-22 18:11:42,729|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /tmp/azureml_runs/torchvision_1608660691_f488dc74\\n2020-12-22 18:11:42,729|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2020-12-22 18:11:42,729|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Storing working dir for pyfs as /tmp/azureml_runs/torchvision_1608660691_f488dc74\\n2020-12-22 18:11:42,735|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[START]\\n2020-12-22 18:11:42,736|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient|DEBUG|ClientBase: Calling batch_create_empty_artifacts with url /artifact/v2.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/artifacts/batch/metadata/{origin}/{container}\\n2020-12-22 18:11:42,988|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[STOP]\\n2020-12-22 18:11:43,494|azureml._history.utils.context_managers.FileWatcher|DEBUG|uploading data to container: azureml blob: ExperimentRun/dcid.torchvision_1608660691_f488dc74/logs/azureml/100222_azureml.log path: /tmp/azureml_runs/torchvision_1608660691_f488dc74/logs/azureml/100222_azureml.log\\n2020-12-22 18:11:43,525|azureml._history.utils.context_managers.FileWatcher|DEBUG|uploading data to container: azureml blob: ExperimentRun/dcid.torchvision_1608660691_f488dc74/logs/azureml/dataprep/python_span_b374fb72-2471-4b1e-ba7e-acce02202b4c.jsonl path: /tmp/azureml_runs/torchvision_1608660691_f488dc74/logs/azureml/dataprep/python_span_b374fb72-2471-4b1e-ba7e-acce02202b4c.jsonl\\n2020-12-22 18:11:43,525|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:11:43,526|azureml._history.utils.context_managers.FileWatcher.UploadQueue.0_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:11:43,527|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 0_result to queue of approximate size: 0\\n2020-12-22 18:12:11,758|azureml.core.authentication|DEBUG|Time to expire 1814361.241467 seconds\\n2020-12-22 18:12:13,530|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:12:13,531|azureml._history.utils.context_managers.FileWatcher.UploadQueue.1_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:12:13,532|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 1_result to queue of approximate size: 1\\n2020-12-22 18:12:41,758|azureml.core.authentication|DEBUG|Time to expire 1814331.241293 seconds\\n2020-12-22 18:12:43,534|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:12:43,534|azureml._history.utils.context_managers.FileWatcher.UploadQueue.2_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:12:43,535|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 2_result to queue of approximate size: 2\\n2020-12-22 18:12:53,537|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:12:53,538|azureml._history.utils.context_managers.FileWatcher.UploadQueue.3_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:12:53,538|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 3_result to queue of approximate size: 3\\n2020-12-22 18:13:11,759|azureml.core.authentication|DEBUG|Time to expire 1814301.240986 seconds\\n2020-12-22 18:13:13,539|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:13:13,539|azureml._history.utils.context_managers.FileWatcher.UploadQueue.4_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:13:13,539|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 4_result to queue of approximate size: 4\\n2020-12-22 18:13:41,759|azureml.core.authentication|DEBUG|Time to expire 1814271.240675 seconds\\n2020-12-22 18:13:43,541|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:13:43,541|azureml._history.utils.context_managers.FileWatcher.UploadQueue.5_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:13:43,541|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 5_result to queue of approximate size: 5\\n2020-12-22 18:14:11,759|azureml.core.authentication|DEBUG|Time to expire 1814241.240361 seconds\\n2020-12-22 18:14:13,543|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:14:13,543|azureml._history.utils.context_managers.FileWatcher.UploadQueue.6_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:14:13,543|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 6_result to queue of approximate size: 6\\n2020-12-22 18:14:41,760|azureml.core.authentication|DEBUG|Time to expire 1814211.240034 seconds\\n2020-12-22 18:14:43,545|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:14:43,545|azureml._history.utils.context_managers.FileWatcher.UploadQueue.7_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:14:43,545|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 7_result to queue of approximate size: 7\\n2020-12-22 18:15:11,760|azureml.core.authentication|DEBUG|Time to expire 1814181.239736 seconds\\n2020-12-22 18:15:13,546|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:15:13,547|azureml._history.utils.context_managers.FileWatcher.UploadQueue.8_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:15:13,547|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 8_result to queue of approximate size: 8\\n2020-12-22 18:15:23,550|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:15:23,550|azureml._history.utils.context_managers.FileWatcher.UploadQueue.9_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:15:23,551|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 9_result to queue of approximate size: 9\\n2020-12-22 18:15:33,553|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:15:33,554|azureml._history.utils.context_managers.FileWatcher.UploadQueue.10_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:15:33,554|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 10_result to queue of approximate size: 10\\n2020-12-22 18:15:41,760|azureml.core.authentication|DEBUG|Time to expire 1814151.239387 seconds\\n2020-12-22 18:15:43,554|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:15:43,555|azureml._history.utils.context_managers.FileWatcher.UploadQueue.11_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:15:43,555|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 11_result to queue of approximate size: 11\\n2020-12-22 18:16:11,761|azureml.core.authentication|DEBUG|Time to expire 1814121.238602 seconds\\n2020-12-22 18:16:13,556|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:16:13,557|azureml._history.utils.context_managers.FileWatcher.UploadQueue.12_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:16:13,557|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 12_result to queue of approximate size: 12\\n2020-12-22 18:16:41,761|azureml.core.authentication|DEBUG|Time to expire 1814091.23829 seconds\\n2020-12-22 18:16:43,558|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:16:43,558|azureml._history.utils.context_managers.FileWatcher.UploadQueue.13_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:16:43,558|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 13_result to queue of approximate size: 13\\n2020-12-22 18:17:11,762|azureml.core.authentication|DEBUG|Time to expire 1814061.238008 seconds\\n2020-12-22 18:17:13,560|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:17:13,560|azureml._history.utils.context_managers.FileWatcher.UploadQueue.14_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:17:13,560|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 14_result to queue of approximate size: 14\\n2020-12-22 18:17:41,762|azureml.core.authentication|DEBUG|Time to expire 1814031.237725 seconds\\n2020-12-22 18:17:43,562|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:17:43,562|azureml._history.utils.context_managers.FileWatcher.UploadQueue.15_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:17:43,562|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 15_result to queue of approximate size: 15\\n2020-12-22 18:18:11,762|azureml.core.authentication|DEBUG|Time to expire 1814001.237422 seconds\\n2020-12-22 18:18:13,563|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:18:13,564|azureml._history.utils.context_managers.FileWatcher.UploadQueue.16_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:18:13,564|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 16_result to queue of approximate size: 16\\n2020-12-22 18:18:41,762|azureml.core.authentication|DEBUG|Time to expire 1813971.237112 seconds\\n2020-12-22 18:18:43,565|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:18:43,566|azureml._history.utils.context_managers.FileWatcher.UploadQueue.17_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:18:43,566|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 17_result to queue of approximate size: 17\\n2020-12-22 18:19:11,763|azureml.core.authentication|DEBUG|Time to expire 1813941.23683 seconds\\n2020-12-22 18:19:13,567|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:19:13,567|azureml._history.utils.context_managers.FileWatcher.UploadQueue.18_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:19:13,568|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 18_result to queue of approximate size: 18\\n2020-12-22 18:19:41,764|azureml.core.authentication|DEBUG|Time to expire 1813911.236027 seconds\\n2020-12-22 18:19:43,569|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:19:43,569|azureml._history.utils.context_managers.FileWatcher.UploadQueue.19_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:19:43,569|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 19_result to queue of approximate size: 19\\n2020-12-22 18:20:11,764|azureml.core.authentication|DEBUG|Time to expire 1813881.235291 seconds\\n2020-12-22 18:20:13,571|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:20:13,572|azureml._history.utils.context_managers.FileWatcher.UploadQueue.20_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:20:13,572|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 20_result to queue of approximate size: 20\\n2020-12-22 18:20:41,764|azureml.core.authentication|DEBUG|Time to expire 1813851.235208 seconds\\n2020-12-22 18:20:43,574|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:20:43,574|azureml._history.utils.context_managers.FileWatcher.UploadQueue.21_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:20:43,574|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 21_result to queue of approximate size: 21\\n2020-12-22 18:20:47,167|azureml._restclient.service_context|DEBUG|Access an existing static threadpool for ServiceContext class\\n2020-12-22 18:20:47,167|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 18:20:47,168|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 18:20:47,168|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 18:20:47,168|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 18:20:47,168|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 18:20:47,168|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 18:20:47,169|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 18:20:47,197|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[START]\\n2020-12-22 18:20:47,197|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.RunClient|DEBUG|ClientBase: Calling get_by_exp_id with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experimentids/{experimentId}/runs/{runId}\\n2020-12-22 18:20:47,264|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[STOP]\\n2020-12-22 18:20:47,264|azureml._SubmittedRun#torchvision_1608660691_f488dc74|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'local', 'ContentSnapshotId': None, 'azureml.git.repository_uri': 'https://github.com/ispmor/azure-project.git', 'mlflow.source.git.repoURL': 'https://github.com/ispmor/azure-project.git', 'azureml.git.branch': 'main', 'mlflow.source.git.branch': 'main', 'azureml.git.commit': '0cd15c57f57b5894818909867bd32604aa9d5ad1', 'mlflow.source.git.commit': '0cd15c57f57b5894818909867bd32604aa9d5ad1', 'azureml.git.dirty': 'True'}\\n2020-12-22 18:20:47,264|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2020-12-22 18:20:47,265|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.MetricsClient|DEBUG|Overrides: Max batch size: 50, batch cushion: 5, Interval: 1.\\n2020-12-22 18:20:47,265|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.PostMetricsBatchV2Daemon|DEBUG|Starting daemon and triggering first instance\\n2020-12-22 18:20:47,265|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.MetricsClient|DEBUG|Used <class 'azureml._common.async_utils.batch_task_queue.BatchTaskQueue'> for use_batch=True.\\n2020-12-22 18:20:47,981|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2020-12-22 18:20:47,981|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /tmp/azureml_runs/torchvision_1608660691_f488dc74\\n2020-12-22 18:20:47,981|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Reverting working dir from /tmp/azureml_runs/torchvision_1608660691_f488dc74 to /tmp/azureml_runs/torchvision_1608660691_f488dc74\\n2020-12-22 18:20:47,981|azureml.history._tracking.PythonWorkingDirectory|INFO|Working dir is already updated /tmp/azureml_runs/torchvision_1608660691_f488dc74\\n2020-12-22 18:20:47,981|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[STOP]\\n2020-12-22 18:20:47,981|azureml.WorkingDirectoryCM|DEBUG|[STOP]\\n2020-12-22 18:20:47,981|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Uploading tracked directories: ['./outputs'], excluding ['azureml-logs/driver_log']\\n2020-12-22 18:20:47,981|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling track for pyfs\\n2020-12-22 18:20:47,981|azureml.history._tracking.PythonWorkingDirectory|DEBUG|./outputs exists as directory, uploading..\\n2020-12-22 18:20:47,981|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Found and adding path to upload: ./outputs/model_latest.pth\\n2020-12-22 18:20:47,981|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Paths to upload is ['./outputs/model_latest.pth'] in dir ./outputs\\n2020-12-22 18:20:47,981|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|Overriding default timeout to 300\\n2020-12-22 18:20:47,981|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|[Start]\\n2020-12-22 18:20:47,982|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[START]\\n2020-12-22 18:20:47,982|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient|DEBUG|ClientBase: Calling batch_create_empty_artifacts with url /artifact/v2.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/artifacts/batch/metadata/{origin}/{container}\\n2020-12-22 18:20:48,129|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[STOP]\\n2020-12-22 18:20:48,129|azureml._restclient.service_context.WorkerPool|DEBUG|submitting future: perform_upload\\n2020-12-22 18:20:48,129|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient.upload_files.0_perform_upload|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:20:48,129|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|Adding task 0_perform_upload to queue of approximate size: 0\\n2020-12-22 18:20:48,129|azureml._restclient.clientbase|DEBUG|ClientBase: Calling create_blob_from_stream with url None\\n2020-12-22 18:20:48,129|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|[Stop] - waiting default timeout\\n2020-12-22 18:20:48,130|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient.upload_files.WaitFlushSource:upload_files|DEBUG|[START]\\n2020-12-22 18:20:48,130|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient.upload_files.WaitFlushSource:upload_files|DEBUG|Overriding default flush timeout from None to 300\\n2020-12-22 18:20:48,130|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient.upload_files.WaitFlushSource:upload_files|DEBUG|Waiting 300 seconds on tasks: [AsyncTask(0_perform_upload)].\\n2020-12-22 18:20:48,266|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|[Start]\\n2020-12-22 18:20:48,266|azureml.BatchTaskQueueAdd_1_Batches.WorkerPool|DEBUG|submitting future: _handle_batch\\n2020-12-22 18:20:48,267|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.MetricsClient.PostMetricsBatchV2|DEBUG|Batch size 1.\\n2020-12-22 18:20:48,267|azureml._restclient.service_context.WorkerPool|DEBUG|submitting future: _log_batch_v2\\n2020-12-22 18:20:48,267|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:20:48,267|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.MetricsClient|DEBUG|Metrics Client: _log_batch_v2 is calling post_run_metrics posting 1 values.\\n2020-12-22 18:20:48,267|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.0__log_batch_v2|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:20:48,267|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|Adding task 0__handle_batch to queue of approximate size: 0\\n2020-12-22 18:20:48,268|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.MetricsClient.post_run_metrics-async:False|DEBUG|[START]\\n2020-12-22 18:20:48,268|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.MetricsClient.PostMetricsBatchV2|DEBUG|Adding task 0__log_batch_v2 to queue of approximate size: 0\\n2020-12-22 18:20:48,268|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|[Stop] - waiting default timeout\\n2020-12-22 18:20:48,268|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.MetricsClient|DEBUG|ClientBase: Calling post_run_metrics with url /metric/v2.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/runs/{runId}/batch\\n2020-12-22 18:20:48,268|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|[START]\\n2020-12-22 18:20:48,272|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|Overriding default flush timeout from None to 120\\n2020-12-22 18:20:48,272|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|Waiting 120 seconds on tasks: [AsyncTask(0__handle_batch)].\\n2020-12-22 18:20:48,272|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:48,272|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch.WaitingTask|DEBUG|Awaiter is BatchTaskQueueAdd_1_Batches\\n2020-12-22 18:20:48,272|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:48,272|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|\\n2020-12-22 18:20:48,272|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|[STOP]\\n2020-12-22 18:20:48,393|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.MetricsClient.post_run_metrics-async:False|DEBUG|[STOP]\\n2020-12-22 18:20:50,488|azureml._file_utils.upload|DEBUG|Uploaded blob ExperimentRun/dcid.torchvision_1608660691_f488dc74/outputs/model_latest.pth with size 176430266.\\n2020-12-22 18:20:50,633|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient.upload_files.0_perform_upload.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,633|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient.upload_files.0_perform_upload.WaitingTask|DEBUG|Awaiter is upload_files\\n2020-12-22 18:20:50,633|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient.upload_files.0_perform_upload.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,634|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|Waiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 8.797645568847656e-05 seconds.\\nWaiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 0.25066590309143066 seconds.\\nWaiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 0.500974178314209 seconds.\\nWaiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 0.7512826919555664 seconds.\\nWaiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 1.0015883445739746 seconds.\\nWaiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 1.2518994808197021 seconds.\\nWaiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 1.5022013187408447 seconds.\\nWaiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 1.7525091171264648 seconds.\\nWaiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 2.0028164386749268 seconds.\\nWaiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 2.2531256675720215 seconds.\\n\\n2020-12-22 18:20:50,634|azureml._SubmittedRun#torchvision_1608660691_f488dc74.RunHistoryFacade.ArtifactsClient.upload_files.WaitFlushSource:upload_files|DEBUG|[STOP]\\n2020-12-22 18:20:50,634|azureml.TrackFolders|DEBUG|[STOP]\\n2020-12-22 18:20:50,634|azureml._history.utils.context_managers|DEBUG|exiting ContentUploader, waiting for file_watcher to finish upload...\\n2020-12-22 18:20:50,634|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher called finish, setting event\\n2020-12-22 18:20:50,634|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher received exit event, getting current_stat\\n2020-12-22 18:20:50,634|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:20:50,635|azureml._history.utils.context_managers.FileWatcher.UploadQueue.22_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:20:50,635|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 22_result to queue of approximate size: 22\\n2020-12-22 18:20:50,635|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher retrieved current_stat, will upload to current_stat\\n2020-12-22 18:20:50,637|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,638|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,638|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,638|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,638|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,639|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,639|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,639|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,639|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,639|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,640|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,640|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,640|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,640|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,640|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,641|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,641|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,641|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,641|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,642|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,642|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,642|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,642|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,642|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 18:20:50,643|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 18:20:50,644|azureml._history.utils.context_managers.FileWatcher.UploadQueue.23_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 18:20:50,644|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 23_result to queue of approximate size: 23\\n2020-12-22 18:20:50,644|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher finished uploading to current_stat, finishing task queue\\n2020-12-22 18:20:50,646|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|[Stop] - waiting default timeout\\n2020-12-22 18:20:50,646|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WaitFlushSource:UploadQueue|DEBUG|[START]\\n2020-12-22 18:20:50,646|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WaitFlushSource:UploadQueue|DEBUG|Overriding default flush timeout from None to 120\\n2020-12-22 18:20:50,646|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WaitFlushSource:UploadQueue|DEBUG|Waiting 120 seconds on tasks: [AsyncTask(0_result), AsyncTask(1_result), AsyncTask(2_result), AsyncTask(3_result), AsyncTask(4_result), AsyncTask(5_result), AsyncTask(6_result), AsyncTask(7_result), AsyncTask(8_result), AsyncTask(9_result), AsyncTask(10_result), AsyncTask(11_result), AsyncTask(12_result), AsyncTask(13_result), AsyncTask(14_result), AsyncTask(15_result), AsyncTask(16_result), AsyncTask(17_result), AsyncTask(18_result), AsyncTask(19_result), AsyncTask(20_result), AsyncTask(21_result), AsyncTask(22_result), AsyncTask(23_result)].\\n2020-12-22 18:20:50,647|azureml._history.utils.context_managers.FileWatcher.UploadQueue.0_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,647|azureml._history.utils.context_managers.FileWatcher.UploadQueue.0_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,647|azureml._history.utils.context_managers.FileWatcher.UploadQueue.0_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,647|azureml._history.utils.context_managers.FileWatcher.UploadQueue.1_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,647|azureml._history.utils.context_managers.FileWatcher.UploadQueue.1_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,647|azureml._history.utils.context_managers.FileWatcher.UploadQueue.1_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,647|azureml._history.utils.context_managers.FileWatcher.UploadQueue.2_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,647|azureml._history.utils.context_managers.FileWatcher.UploadQueue.2_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,647|azureml._history.utils.context_managers.FileWatcher.UploadQueue.2_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,647|azureml._history.utils.context_managers.FileWatcher.UploadQueue.3_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,647|azureml._history.utils.context_managers.FileWatcher.UploadQueue.3_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,647|azureml._history.utils.context_managers.FileWatcher.UploadQueue.3_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,647|azureml._history.utils.context_managers.FileWatcher.UploadQueue.4_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,647|azureml._history.utils.context_managers.FileWatcher.UploadQueue.4_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,647|azureml._history.utils.context_managers.FileWatcher.UploadQueue.4_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.5_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.5_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.5_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.6_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.6_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.6_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.7_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.7_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.7_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.8_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.8_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.8_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.9_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.9_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.9_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.10_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.10_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.10_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.11_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.11_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.11_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.12_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,648|azureml._history.utils.context_managers.FileWatcher.UploadQueue.12_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.12_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.13_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.13_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.13_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.14_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.14_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.14_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.15_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.15_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.15_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.16_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.16_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.16_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.17_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.17_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.17_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.18_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.18_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.18_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.19_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.19_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.19_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,649|azureml._history.utils.context_managers.FileWatcher.UploadQueue.20_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,650|azureml._history.utils.context_managers.FileWatcher.UploadQueue.20_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,650|azureml._history.utils.context_managers.FileWatcher.UploadQueue.20_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,650|azureml._history.utils.context_managers.FileWatcher.UploadQueue.21_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,650|azureml._history.utils.context_managers.FileWatcher.UploadQueue.21_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,650|azureml._history.utils.context_managers.FileWatcher.UploadQueue.21_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,650|azureml._history.utils.context_managers.FileWatcher.UploadQueue.22_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,650|azureml._history.utils.context_managers.FileWatcher.UploadQueue.22_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,650|azureml._history.utils.context_managers.FileWatcher.UploadQueue.22_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,900|azureml._history.utils.context_managers.FileWatcher.UploadQueue.23_result.WaitingTask|DEBUG|[START]\\n2020-12-22 18:20:50,900|azureml._history.utils.context_managers.FileWatcher.UploadQueue.23_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 18:20:50,901|azureml._history.utils.context_managers.FileWatcher.UploadQueue.23_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 18:20:50,901|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Waiting on task: 23_result.\\n1 tasks left. Current duration of flush 0.003550291061401367 seconds.\\n\\n2020-12-22 18:20:50,901|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WaitFlushSource:UploadQueue|DEBUG|[STOP]\\n\\nRun is completed.\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.17.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = exp.submit(estimator)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: torchvision_1608660691_f488dc74\n",
      "Web View: https://ml.azure.com/experiments/torchvision/runs/torchvision_1608660691_f488dc74?wsid=/subscriptions/1db0a5ce-7de1-4082-8e25-3c5a4e5a9a98/resourcegroups/ProjektAzure/workspaces/ProjektAzure\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "[2020-12-22T18:11:39.566396] Entering context manager injector.\n",
      "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['train.py', '--data_path', '.', '--workers', '8', '--learning_rate', '0.005', '--epochs', '1', '--anchor_sizes', '16,32,64,128,256,512', '--anchor_aspect_ratios', '0.25,0.5,1.0,2.0', '--rpn_nms_thresh', '0.5', '--box_nms_thresh', '0.3', '--box_score_thresh', '0.1'])\n",
      "Script type = None\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 100222\n",
      "Entering Run History Context Manager.\n",
      "[2020-12-22T18:11:42.972997] Current directory: /tmp/azureml_runs/torchvision_1608660691_f488dc74\n",
      "[2020-12-22T18:11:42.973024] Preparing to call script [train.py] with arguments:['--data_path', '.', '--workers', '8', '--learning_rate', '0.005', '--epochs', '1', '--anchor_sizes', '16,32,64,128,256,512', '--anchor_aspect_ratios', '0.25,0.5,1.0,2.0', '--rpn_nms_thresh', '0.5', '--box_nms_thresh', '0.3', '--box_score_thresh', '0.1']\n",
      "[2020-12-22T18:11:42.973040] After variable expansion, calling script [train.py] with arguments:['--data_path', '.', '--workers', '8', '--learning_rate', '0.005', '--epochs', '1', '--anchor_sizes', '16,32,64,128,256,512', '--anchor_aspect_ratios', '0.25,0.5,1.0,2.0', '--rpn_nms_thresh', '0.5', '--box_nms_thresh', '0.3', '--box_score_thresh', '0.1']\n",
      "\n",
      "Epoch: [0]  [  0/279]  eta: 0:11:23  lr: 0.000023  loss: 1.4584 (1.4584)  loss_classifier: 0.5685 (0.5685)  loss_box_reg: 0.0059 (0.0059)  loss_objectness: 0.6834 (0.6834)  loss_rpn_box_reg: 0.2006 (0.2006)  time: 2.4508  data: 0.3096  max mem: 2711\n",
      "Epoch: [0]  [ 10/279]  eta: 0:07:51  lr: 0.000203  loss: 1.1796 (1.2609)  loss_classifier: 0.4127 (0.3926)  loss_box_reg: 0.0090 (0.0110)  loss_objectness: 0.6896 (0.6902)  loss_rpn_box_reg: 0.1853 (0.1671)  time: 1.7527  data: 0.0328  max mem: 4056\n",
      "Epoch: [0]  [ 20/279]  eta: 0:07:21  lr: 0.000382  loss: 1.0137 (1.1038)  loss_classifier: 0.1190 (0.2441)  loss_box_reg: 0.0146 (0.0159)  loss_objectness: 0.6875 (0.6874)  loss_rpn_box_reg: 0.1619 (0.1564)  time: 1.6655  data: 0.0043  max mem: 4056\n",
      "Epoch: [0]  [ 30/279]  eta: 0:07:09  lr: 0.000562  loss: 0.9961 (1.0967)  loss_classifier: 0.0999 (0.2156)  loss_box_reg: 0.0305 (0.0342)  loss_objectness: 0.6785 (0.6831)  loss_rpn_box_reg: 0.1619 (0.1638)  time: 1.7125  data: 0.0049  max mem: 4204\n",
      "Epoch: [0]  [ 40/279]  eta: 0:06:53  lr: 0.000742  loss: 1.0458 (1.0741)  loss_classifier: 0.1451 (0.1962)  loss_box_reg: 0.0664 (0.0493)  loss_objectness: 0.6721 (0.6793)  loss_rpn_box_reg: 0.1283 (0.1493)  time: 1.7584  data: 0.0061  max mem: 4204\n",
      "Epoch: [0]  [ 50/279]  eta: 0:06:38  lr: 0.000921  loss: 1.0014 (1.0648)  loss_classifier: 0.1145 (0.1831)  loss_box_reg: 0.1066 (0.0633)  loss_objectness: 0.6634 (0.6740)  loss_rpn_box_reg: 0.0963 (0.1444)  time: 1.7626  data: 0.0061  max mem: 4497\n",
      "Epoch: [0]  [ 60/279]  eta: 0:06:24  lr: 0.001101  loss: 0.9715 (1.0508)  loss_classifier: 0.1097 (0.1725)  loss_box_reg: 0.1282 (0.0755)  loss_objectness: 0.6352 (0.6655)  loss_rpn_box_reg: 0.1059 (0.1373)  time: 1.8052  data: 0.0062  max mem: 4497\n",
      "Epoch: [0]  [ 70/279]  eta: 0:06:06  lr: 0.001281  loss: 0.9856 (1.0440)  loss_classifier: 0.1236 (0.1665)  loss_box_reg: 0.1889 (0.0951)  loss_objectness: 0.5814 (0.6508)  loss_rpn_box_reg: 0.0973 (0.1316)  time: 1.7931  data: 0.0063  max mem: 4497\n",
      "Epoch: [0]  [ 80/279]  eta: 0:05:49  lr: 0.001460  loss: 0.9737 (1.0300)  loss_classifier: 0.0970 (0.1571)  loss_box_reg: 0.2244 (0.1066)  loss_objectness: 0.5609 (0.6384)  loss_rpn_box_reg: 0.1015 (0.1279)  time: 1.7618  data: 0.0062  max mem: 4497\n",
      "Epoch: [0]  [ 90/279]  eta: 0:05:33  lr: 0.001640  loss: 0.8494 (1.0125)  loss_classifier: 0.0686 (0.1487)  loss_box_reg: 0.1787 (0.1168)  loss_objectness: 0.5033 (0.6178)  loss_rpn_box_reg: 0.1169 (0.1292)  time: 1.8001  data: 0.0062  max mem: 4497\n",
      "Epoch: [0]  [100/279]  eta: 0:05:15  lr: 0.001820  loss: 0.8418 (0.9996)  loss_classifier: 0.0686 (0.1427)  loss_box_reg: 0.1829 (0.1277)  loss_objectness: 0.4292 (0.5986)  loss_rpn_box_reg: 0.1170 (0.1307)  time: 1.7778  data: 0.0063  max mem: 4497\n",
      "Epoch: [0]  [110/279]  eta: 0:04:59  lr: 0.001999  loss: 0.8022 (0.9745)  loss_classifier: 0.0694 (0.1351)  loss_box_reg: 0.1762 (0.1296)  loss_objectness: 0.3889 (0.5778)  loss_rpn_box_reg: 0.1263 (0.1319)  time: 1.7908  data: 0.0065  max mem: 4497\n",
      "Epoch: [0]  [120/279]  eta: 0:04:41  lr: 0.002179  loss: 0.7216 (0.9555)  loss_classifier: 0.0539 (0.1307)  loss_box_reg: 0.1565 (0.1322)  loss_objectness: 0.3319 (0.5551)  loss_rpn_box_reg: 0.1743 (0.1375)  time: 1.8183  data: 0.0067  max mem: 4497\n",
      "Epoch: [0]  [130/279]  eta: 0:04:23  lr: 0.002359  loss: 0.7165 (0.9361)  loss_classifier: 0.0748 (0.1273)  loss_box_reg: 0.1738 (0.1400)  loss_objectness: 0.2618 (0.5297)  loss_rpn_box_reg: 0.1836 (0.1390)  time: 1.7806  data: 0.0066  max mem: 4497\n",
      "Epoch: [0]  [140/279]  eta: 0:04:06  lr: 0.002538  loss: 0.6372 (0.9106)  loss_classifier: 0.0852 (0.1247)  loss_box_reg: 0.2202 (0.1460)  loss_objectness: 0.1586 (0.5009)  loss_rpn_box_reg: 0.1396 (0.1389)  time: 1.8025  data: 0.0062  max mem: 4497\n",
      "Epoch: [0]  [150/279]  eta: 0:03:49  lr: 0.002718  loss: 0.4851 (0.8850)  loss_classifier: 0.0776 (0.1220)  loss_box_reg: 0.2081 (0.1500)  loss_objectness: 0.1072 (0.4748)  loss_rpn_box_reg: 0.1304 (0.1381)  time: 1.8341  data: 0.0063  max mem: 4497\n",
      "Epoch: [0]  [160/279]  eta: 0:03:32  lr: 0.002898  loss: 0.4043 (0.8536)  loss_classifier: 0.0559 (0.1181)  loss_box_reg: 0.2046 (0.1506)  loss_objectness: 0.0808 (0.4494)  loss_rpn_box_reg: 0.1066 (0.1354)  time: 1.8409  data: 0.0065  max mem: 4497\n",
      "Epoch: [0]  [170/279]  eta: 0:03:14  lr: 0.003077  loss: 0.4043 (0.8308)  loss_classifier: 0.0495 (0.1154)  loss_box_reg: 0.1563 (0.1523)  loss_objectness: 0.0665 (0.4282)  loss_rpn_box_reg: 0.1037 (0.1349)  time: 1.7842  data: 0.0064  max mem: 4497\n",
      "Epoch: [0]  [180/279]  eta: 0:02:55  lr: 0.003257  loss: 0.3553 (0.8046)  loss_classifier: 0.0546 (0.1121)  loss_box_reg: 0.1482 (0.1524)  loss_objectness: 0.0660 (0.4075)  loss_rpn_box_reg: 0.1015 (0.1326)  time: 1.7332  data: 0.0062  max mem: 4497\n",
      "Epoch: [0]  [190/279]  eta: 0:02:37  lr: 0.003437  loss: 0.3473 (0.7789)  loss_classifier: 0.0529 (0.1088)  loss_box_reg: 0.1299 (0.1510)  loss_objectness: 0.0402 (0.3882)  loss_rpn_box_reg: 0.0924 (0.1308)  time: 1.7268  data: 0.0062  max mem: 4497\n",
      "Epoch: [0]  [200/279]  eta: 0:02:20  lr: 0.003616  loss: 0.3405 (0.7589)  loss_classifier: 0.0556 (0.1068)  loss_box_reg: 0.1557 (0.1517)  loss_objectness: 0.0453 (0.3715)  loss_rpn_box_reg: 0.0871 (0.1288)  time: 1.7307  data: 0.0062  max mem: 4497\n",
      "Epoch: [0]  [210/279]  eta: 0:02:02  lr: 0.003796  loss: 0.3661 (0.7442)  loss_classifier: 0.0639 (0.1049)  loss_box_reg: 0.1597 (0.1531)  loss_objectness: 0.0532 (0.3576)  loss_rpn_box_reg: 0.0981 (0.1285)  time: 1.7540  data: 0.0061  max mem: 4497\n",
      "Epoch: [0]  [220/279]  eta: 0:01:44  lr: 0.003976  loss: 0.4710 (0.7311)  loss_classifier: 0.0662 (0.1032)  loss_box_reg: 0.2117 (0.1560)  loss_objectness: 0.0641 (0.3442)  loss_rpn_box_reg: 0.1065 (0.1277)  time: 1.7933  data: 0.0064  max mem: 4497\n",
      "Epoch: [0]  [230/279]  eta: 0:01:27  lr: 0.004156  loss: 0.4793 (0.7186)  loss_classifier: 0.0687 (0.1018)  loss_box_reg: 0.2524 (0.1590)  loss_objectness: 0.0633 (0.3318)  loss_rpn_box_reg: 0.0948 (0.1259)  time: 1.8130  data: 0.0065  max mem: 4497\n",
      "Epoch: [0]  [240/279]  eta: 0:01:09  lr: 0.004335  loss: 0.4356 (0.7050)  loss_classifier: 0.0631 (0.1001)  loss_box_reg: 0.2233 (0.1606)  loss_objectness: 0.0498 (0.3196)  loss_rpn_box_reg: 0.0892 (0.1248)  time: 1.8209  data: 0.0066  max mem: 4497\n",
      "Epoch: [0]  [250/279]  eta: 0:00:51  lr: 0.004515  loss: 0.3055 (0.6930)  loss_classifier: 0.0446 (0.0979)  loss_box_reg: 0.1156 (0.1598)  loss_objectness: 0.0393 (0.3094)  loss_rpn_box_reg: 0.0892 (0.1260)  time: 1.8073  data: 0.0066  max mem: 4497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [260/279]  eta: 0:00:33  lr: 0.004695  loss: 0.2983 (0.6817)  loss_classifier: 0.0397 (0.0965)  loss_box_reg: 0.0871 (0.1604)  loss_objectness: 0.0609 (0.3000)  loss_rpn_box_reg: 0.0903 (0.1248)  time: 1.7745  data: 0.0063  max mem: 4529\n",
      "Epoch: [0]  [270/279]  eta: 0:00:16  lr: 0.004874  loss: 0.3665 (0.6713)  loss_classifier: 0.0568 (0.0952)  loss_box_reg: 0.1667 (0.1609)  loss_objectness: 0.0528 (0.2909)  loss_rpn_box_reg: 0.0903 (0.1243)  time: 1.7837  data: 0.0066  max mem: 4529\n",
      "Epoch: [0]  [278/279]  eta: 0:00:01  lr: 0.005000  loss: 0.3725 (0.6658)  loss_classifier: 0.0600 (0.0949)  loss_box_reg: 0.1916 (0.1628)  loss_objectness: 0.0528 (0.2843)  loss_rpn_box_reg: 0.1070 (0.1239)  time: 1.7413  data: 0.0064  max mem: 4529\n",
      "Epoch: [0] Total time: 0:08:15 (1.7767 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "<string>:6: DeprecationWarning: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n",
      "Test:  [ 0/50]  eta: 0:00:58  model_time: 0.9198 (0.9198)  evaluator_time: 0.0202 (0.0202)  time: 1.1684  data: 0.2241  max mem: 4529\n",
      "Test:  [49/50]  eta: 0:00:00  model_time: 0.8117 (0.7842)  evaluator_time: 0.0071 (0.0097)  time: 0.8012  data: 0.0064  max mem: 4529\n",
      "Test: Total time: 0:00:40 (0.8072 s / it)\n",
      "Averaged stats: model_time: 0.8117 (0.7842)  evaluator_time: 0.0071 (0.0097)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.534\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.977\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.509\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.316\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.547\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.533\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.084\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.463\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.602\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.353\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.615\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.600\n",
      "That's it!\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 100222\n",
      "\n",
      "\n",
      "[2020-12-22T18:20:47.776764] The experiment completed successfully. Finalizing run...\n",
      "[2020-12-22T18:20:47.776783] Start FinalizingInRunHistory\n",
      "[2020-12-22T18:20:47.777287] Logging experiment finalizing status in history service.\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.10217523574829102 seconds\n",
      "[2020-12-22T18:20:51.331392] Finished context manager injector.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: torchvision_1608660691_f488dc74\n",
      "Web View: https://ml.azure.com/experiments/torchvision/runs/torchvision_1608660691_f488dc74?wsid=/subscriptions/1db0a5ce-7de1-4082-8e25-3c5a4e5a9a98/resourcegroups/ProjektAzure/workspaces/ProjektAzure\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'torchvision_1608660691_f488dc74',\n",
       " 'target': 'local',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2020-12-22T18:11:38.281837Z',\n",
       " 'endTimeUtc': '2020-12-22T18:20:54.064672Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'local',\n",
       "  'ContentSnapshotId': None,\n",
       "  'azureml.git.repository_uri': 'https://github.com/ispmor/azure-project.git',\n",
       "  'mlflow.source.git.repoURL': 'https://github.com/ispmor/azure-project.git',\n",
       "  'azureml.git.branch': 'main',\n",
       "  'mlflow.source.git.branch': 'main',\n",
       "  'azureml.git.commit': '0cd15c57f57b5894818909867bd32604aa9d5ad1',\n",
       "  'mlflow.source.git.commit': '0cd15c57f57b5894818909867bd32604aa9d5ad1',\n",
       "  'azureml.git.dirty': 'True'},\n",
       " 'inputDatasets': [],\n",
       " 'outputDatasets': [],\n",
       " 'runDefinition': {'script': 'train.py',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': ['--data_path',\n",
       "   '.',\n",
       "   '--workers',\n",
       "   '8',\n",
       "   '--learning_rate',\n",
       "   '0.005',\n",
       "   '--epochs',\n",
       "   '1',\n",
       "   '--anchor_sizes',\n",
       "   '16,32,64,128,256,512',\n",
       "   '--anchor_aspect_ratios',\n",
       "   '0.25,0.5,1.0,2.0',\n",
       "   '--rpn_nms_thresh',\n",
       "   '0.5',\n",
       "   '--box_nms_thresh',\n",
       "   '0.3',\n",
       "   '--box_score_thresh',\n",
       "   '0.1'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'local',\n",
       "  'dataReferences': {},\n",
       "  'data': {},\n",
       "  'outputData': {},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'priority': None,\n",
       "  'credentialPassthrough': False,\n",
       "  'environment': {'name': 'Experiment torchvision Environment',\n",
       "   'version': 'Autosave_2020-12-21T20:57:49Z_6cb444b5',\n",
       "   'python': {'interpreterPath': '/anaconda/envs/azureml_py36_pytorch/bin/python',\n",
       "    'userManagedDependencies': True,\n",
       "    'condaDependencies': {'channels': ['anaconda', 'conda-forge'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-defaults',\n",
       "        'torch==1.4.0',\n",
       "        'torchvision==0.5.0',\n",
       "        'horovod==0.18.1',\n",
       "        'tensorboard==1.14.0',\n",
       "        'future==0.17.1']}],\n",
       "     'name': 'project_environment'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE',\n",
       "    'NCCL_TREE_THRESHOLD': '0'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': False,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': False},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'enableMLflowTracking': True,\n",
       "   'snapshotProject': False},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'aiSuperComputer': {'instanceType': None,\n",
       "   'frameworkImage': None,\n",
       "   'imageVersion': None,\n",
       "   'location': None,\n",
       "   'aiSuperComputerStorageData': None},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': False,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'commandReturnCodeConfig': {'returnCode': 'Zero',\n",
       "   'successfulReturnCodes': []}},\n",
       " 'logFiles': {'azureml-logs/60_control_log.txt': 'https://projektastorage1d32b8c8a.blob.core.windows.net/azureml/ExperimentRun/dcid.torchvision_1608660691_f488dc74/azureml-logs/60_control_log.txt?sv=2019-02-02&sr=b&sig=yUC3nXNiGcTHMceBEPqEqPTtEqzstnkjJsp%2F2g2b7WQ%3D&st=2020-12-22T18%3A10%3A56Z&se=2020-12-23T02%3A20%3A56Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://projektastorage1d32b8c8a.blob.core.windows.net/azureml/ExperimentRun/dcid.torchvision_1608660691_f488dc74/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=H7tkw8B3qRgU0AsquXfPeNZEnqQHPay0tHayWLjugVM%3D&st=2020-12-22T18%3A10%3A56Z&se=2020-12-23T02%3A20%3A56Z&sp=r',\n",
       "  'logs/azureml/100222_azureml.log': 'https://projektastorage1d32b8c8a.blob.core.windows.net/azureml/ExperimentRun/dcid.torchvision_1608660691_f488dc74/logs/azureml/100222_azureml.log?sv=2019-02-02&sr=b&sig=ySjguslG0Ix783tZdllx1RLpQQwlehkR8iKuM565yBA%3D&st=2020-12-22T18%3A01%3A45Z&se=2020-12-23T02%3A11%3A45Z&sp=r',\n",
       "  'logs/azureml/dataprep/python_span_b374fb72-2471-4b1e-ba7e-acce02202b4c.jsonl': 'https://projektastorage1d32b8c8a.blob.core.windows.net/azureml/ExperimentRun/dcid.torchvision_1608660691_f488dc74/logs/azureml/dataprep/python_span_b374fb72-2471-4b1e-ba7e-acce02202b4c.jsonl?sv=2019-02-02&sr=b&sig=NASDltquOK7yHjFdesxBD%2BFYbSjj7PBqIqBsp6UwBTo%3D&st=2020-12-22T18%3A01%3A47Z&se=2020-12-23T02%3A11%3A47Z&sp=r'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azureml-logs/60_control_log.txt',\n",
       " 'azureml-logs/70_driver_log.txt',\n",
       " 'logs/azureml/100222_azureml.log',\n",
       " 'logs/azureml/dataprep/python_span_b374fb72-2471-4b1e-ba7e-acce02202b4c.jsonl',\n",
       " 'outputs/model_latest.pth']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mAP@IoU=0.50': 0.977188553048049}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='ProjektAzure', subscription_id='1db0a5ce-7de1-4082-8e25-3c5a4e5a9a98', resource_group='ProjektAzure'), name=torchvision_local_model, id=torchvision_local_model:3, version=3, tags={}, properties={})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.register_model(model_name=\"torchvision_local_model\", model_path=\"/outputs/model_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file(\"outputs/model_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "anchor_sizes = \"16,32,64,128,256,512\"\n",
    "anchor_aspect_ratios = \"0.25,0.5,1.0,2.0\"\n",
    "rpn_nms_threshold = 0.5\n",
    "box_nms_threshold = 0.3\n",
    "box_score_threshold = 0.1\n",
    "num_box_detections = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Mask RCNN model\n",
    "model = get_model(\n",
    "    num_classes,\n",
    "    anchor_sizes,\n",
    "    anchor_aspect_ratios,\n",
    "    rpn_nms_threshold,\n",
    "    box_nms_threshold,\n",
    "    box_score_threshold,\n",
    "    num_box_detections,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign()\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "    (mask_roi_pool): None\n",
       "    (mask_head): MaskRCNNHeads(\n",
       "      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu2): ReLU(inplace=True)\n",
       "      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu3): ReLU(inplace=True)\n",
       "      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu4): ReLU(inplace=True)\n",
       "    )\n",
       "    (mask_predictor): MaskRCNNPredictor(\n",
       "      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask_fcn_logits): Conv2d(256, 91, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"model_latest.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a random subset of the data to visualize predictions on the images.\n",
    "data_path = \"./scripts\"\n",
    "dataset = BuildDataset(data_path, get_transform(train=False))\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(dataset)):\n",
    "#     img, _ = dataset[i]\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         prediction = model([img.to(device)])\n",
    "#     img = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
    "#     preds = prediction[0][\"boxes\"].cpu().numpy()\n",
    "#     print(prediction[0][\"scores\"])\n",
    "#     draw = ImageDraw.Draw(img)\n",
    "#     for i in range(len(preds)):\n",
    "#         draw.rectangle(\n",
    "#             ((preds[i][0], preds[i][1]), (preds[i][2], preds[i][3])), outline=\"red\"\n",
    "#         )\n",
    "#     display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

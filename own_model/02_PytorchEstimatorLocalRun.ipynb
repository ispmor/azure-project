{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/training-with-deep-learning/how-to-use-estimator/how-to-use-estimator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Run Using Pytorch Estimator in Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use Azure ML's PyTorch estimator to run our training script locally by using the conda environment created for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.17.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"scripts\")\n",
    "sys.path.append(\"scripts/cocoapi/PythonAPI/\")\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.dnn import PyTorch\n",
    "\n",
    "from dotenv import set_key, get_key, find_dotenv\n",
    "from utilities import get_auth, download_data\n",
    "\n",
    "import torch\n",
    "from scripts.XMLDataset import BuildDataset, get_transform\n",
    "from scripts.maskrcnn_model import get_model\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = find_dotenv(raise_error_if_not_found=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first download the dataset that includes the images of store shelves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Finished extracting.\n"
     ]
    }
   ],
   "source": [
    "data_file = \"Data.zip\"\n",
    "data_url = (\"https://bostondata.blob.core.windows.net/builddata/{}\".format(data_file))\n",
    "download_data(data_file, data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Let's load the existing workspace you created earlier in the Azure ML configuration notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProjektAzure\n",
      "ProjektAzure\n",
      "eastus\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config(auth=get_auth(env_path))\n",
    "print(ws.name, ws.resource_group, ws.location, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Azure ML experiment\n",
    "Let's create an experiment and give it a name. The script runs will be recorded under this experiment in Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(workspace=ws, name='torchvision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a train.py script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import sys\n",
      "\n",
      "sys.path.append(\"./cocoapi/PythonAPI/\")\n",
      "\n",
      "import torch\n",
      "import argparse\n",
      "import utils\n",
      "from XMLDataset import BuildDataset, get_transform\n",
      "from maskrcnn_model import get_model\n",
      "from engine import train_one_epoch, evaluate\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    parser = argparse.ArgumentParser(description=\"PyTorch Object Detection Training\")\n",
      "    parser.add_argument(\n",
      "        \"--data_path\", default=\"./Data/\", help=\"the path to the dataset\"\n",
      "    )\n",
      "    parser.add_argument(\"--batch_size\", default=2, type=int)\n",
      "    parser.add_argument(\n",
      "        \"--epochs\", default=10, type=int, help=\"number of total epochs to run\"\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--workers\", default=4, type=int, help=\"number of data loading workers\"\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--learning_rate\", default=0.005, type=float, help=\"initial learning rate\"\n",
      "    )\n",
      "    parser.add_argument(\"--momentum\", default=0.9, type=float, help=\"momentum\")\n",
      "    parser.add_argument(\n",
      "        \"--weight_decay\",\n",
      "        default=0.0005,\n",
      "        type=float,\n",
      "        help=\"weight decay (default: 1e-4)\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--lr_step_size\", default=3, type=int, help=\"decrease lr every step-size epochs\"\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--lr_gamma\",\n",
      "        default=0.1,\n",
      "        type=float,\n",
      "        help=\"decrease lr by a factor of lr-gamma\",\n",
      "    )\n",
      "    parser.add_argument(\"--print_freq\", default=10, type=int, help=\"print frequency\")\n",
      "    parser.add_argument(\"--output_dir\", default=\"outputs\", help=\"path where to save\")\n",
      "    parser.add_argument(\"--anchor_sizes\", default=\"16\", type=str, help=\"anchor sizes\")\n",
      "    parser.add_argument(\n",
      "        \"--anchor_aspect_ratios\", default=\"1.0\", type=str, help=\"anchor aspect ratios\"\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--rpn_nms_thresh\",\n",
      "        default=0.7,\n",
      "        type=float,\n",
      "        help=\"NMS threshold used for postprocessing the RPN proposals\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--box_nms_thresh\",\n",
      "        default=0.5,\n",
      "        type=float,\n",
      "        help=\"NMS threshold for the prediction head. Used during inference\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--box_score_thresh\",\n",
      "        default=0.05,\n",
      "        type=float,\n",
      "        help=\"during inference only return proposals\"\n",
      "        \"with a classification score greater than box_score_thresh\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--box_detections_per_img\",\n",
      "        default=100,\n",
      "        type=int,\n",
      "        help=\"maximum number of detections per image, for all classes\",\n",
      "    )\n",
      "    args = parser.parse_args()\n",
      "\n",
      "data_path = args.data_path\n",
      "\n",
      "# use our dataset and defined transformations\n",
      "dataset = BuildDataset(data_path, get_transform(train=True))\n",
      "dataset_test = BuildDataset(data_path, get_transform(train=False))\n",
      "\n",
      "# split the dataset in train and test set\n",
      "indices = torch.randperm(len(dataset)).tolist()\n",
      "dataset = torch.utils.data.Subset(dataset, indices[:-100])\n",
      "dataset_test = torch.utils.data.Subset(dataset_test, indices[-100:])\n",
      "\n",
      "batch_size = args.batch_size\n",
      "workers = args.workers\n",
      "\n",
      "# define training and validation data loaders\n",
      "data_loader = torch.utils.data.DataLoader(\n",
      "    dataset,\n",
      "    batch_size=2,\n",
      "    shuffle=True,\n",
      "    num_workers=workers,\n",
      "    collate_fn=utils.collate_fn,\n",
      ")\n",
      "\n",
      "data_loader_test = torch.utils.data.DataLoader(\n",
      "    dataset_test,\n",
      "    batch_size=2,\n",
      "    shuffle=False,\n",
      "    num_workers=workers,\n",
      "    collate_fn=utils.collate_fn,\n",
      ")\n",
      "\n",
      "# our dataset has two classes only - background and out of stock\n",
      "num_classes = 2\n",
      "\n",
      "model = get_model(\n",
      "    num_classes,\n",
      "    args.anchor_sizes,\n",
      "    args.anchor_aspect_ratios,\n",
      "    args.rpn_nms_thresh,\n",
      "    args.box_nms_thresh,\n",
      "    args.box_score_thresh,\n",
      "    args.box_detections_per_img,\n",
      ")\n",
      "\n",
      "# train on the GPU or on the CPU, if a GPU is not available\n",
      "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
      "\n",
      "# move model to the right device\n",
      "model.to(device)\n",
      "\n",
      "learning_rate = args.learning_rate\n",
      "momentum = args.momentum\n",
      "weight_decay = args.weight_decay\n",
      "\n",
      "# construct an optimizer\n",
      "params = [p for p in model.parameters() if p.requires_grad]\n",
      "optimizer = torch.optim.SGD(\n",
      "    params, lr=learning_rate, momentum=momentum, weight_decay=weight_decay\n",
      ")\n",
      "\n",
      "lr_step_size = args.lr_step_size\n",
      "lr_gamma = args.lr_gamma\n",
      "\n",
      "# and a learning rate scheduler\n",
      "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
      "    optimizer, step_size=lr_step_size, gamma=lr_gamma\n",
      ")\n",
      "\n",
      "# number of training epochs\n",
      "num_epochs = args.epochs\n",
      "print_freq = args.print_freq\n",
      "\n",
      "for epoch in range(num_epochs):\n",
      "    # train for one epoch, printing every 10 iterations\n",
      "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=print_freq)\n",
      "    # update the learning rate\n",
      "    lr_scheduler.step()\n",
      "    # evaluate on the test dataset after every epoch\n",
      "    evaluate(model, data_loader_test, device=device)\n",
      "\n",
      "# save model\n",
      "if not os.path.exists(args.output_dir):\n",
      "    os.makedirs(args.output_dir)\n",
      "torch.save(model.state_dict(), os.path.join(args.output_dir, \"model_latest.pth\"))\n",
      "\n",
      "print(\"That's it!\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"scripts/train.py\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create A Pytorch Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we pick the number of epochs to run the training for.This deliberately has a low default value for the speed of running. In actual application, set this to higher values (i.e. num_epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 'use_docker' parameter will be deprecated. Please use 'environment_definition' instead.\n"
     ]
    }
   ],
   "source": [
    "script_params = {\n",
    "    \"--data_path\": \".\",\n",
    "    \"--workers\": 8,\n",
    "    \"--learning_rate\": 0.005,\n",
    "    \"--epochs\": num_epochs,\n",
    "    \"--anchor_sizes\": \"16,32,64,128,256,512\",\n",
    "    \"--anchor_aspect_ratios\": \"0.25,0.5,1.0,2.0\",\n",
    "    \"--rpn_nms_thresh\": 0.5,\n",
    "    \"--box_nms_thresh\": 0.3,\n",
    "    \"--box_score_thresh\": 0.10,\n",
    "}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    source_directory=\"./scripts\",\n",
    "    script_params=script_params,\n",
    "    compute_target=\"local\",\n",
    "    entry_script=\"train.py\",\n",
    "    use_docker=False,\n",
    "    user_managed=True,\n",
    "    use_gpu=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we point the python interpreter to the local conda environment built for this tutorial. Azure ML SDK will run the training script using this environment. We also turn off project snapshot upload to the cloud since we have a large dataset in the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.run_config.environment.python.interpreter_path = (\"/anaconda/envs/azureml_py36_pytorch/bin/python\")\n",
    "estimator.run_config.history.snapshot_project = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - If 'script' has been provided here and a script file name has been specified in 'run_config', 'script' provided in ScriptRunConfig initialization will take precedence.\n",
      "WARNING - If 'arguments' has been provided here and arguments have been specified in 'run_config', 'arguments' provided in ScriptRunConfig initialization will take precedence.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70360e5db2a4b7a8d9110ab71134745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/torchvision/runs/torchvision_1608655769_f46fb20e?wsid=/subscriptions/1db0a5ce-7de1-4082-8e25-3c5a4e5a9a98/resourcegroups/ProjektAzure/workspaces/ProjektAzure\", \"run_id\": \"torchvision_1608655769_f46fb20e\", \"run_properties\": {\"run_id\": \"torchvision_1608655769_f46fb20e\", \"created_utc\": \"2020-12-22T16:49:30.853429Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"local\", \"ContentSnapshotId\": null, \"azureml.git.repository_uri\": \"https://github.com/microsoft/HyperdriveDeepLearning.git\", \"mlflow.source.git.repoURL\": \"https://github.com/microsoft/HyperdriveDeepLearning.git\", \"azureml.git.branch\": \"master\", \"mlflow.source.git.branch\": \"master\", \"azureml.git.commit\": \"50577d753e96bc0b409215a08a544bce8bc7027b\", \"mlflow.source.git.commit\": \"50577d753e96bc0b409215a08a544bce8bc7027b\", \"azureml.git.dirty\": \"True\"}, \"tags\": {\"mlflow.source.type\": \"JOB\", \"mlflow.source.name\": \"train.py\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2020-12-22T16:58:48.558004Z\", \"status\": \"Completed\", \"log_files\": {\"azureml-logs/60_control_log.txt\": \"https://projektastorage1d32b8c8a.blob.core.windows.net/azureml/ExperimentRun/dcid.torchvision_1608655769_f46fb20e/azureml-logs/60_control_log.txt?sv=2019-02-02&sr=b&sig=KCg7lEDWctWFWgGqoCmLZfhIrTIYHW5CyHNv01n2dPs%3D&st=2020-12-22T16%3A49%3A00Z&se=2020-12-23T00%3A59%3A00Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://projektastorage1d32b8c8a.blob.core.windows.net/azureml/ExperimentRun/dcid.torchvision_1608655769_f46fb20e/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=NyTNtcdlMEfaFxFE77VY2ITl7usTTfUmGuwAueSVg9Q%3D&st=2020-12-22T16%3A49%3A00Z&se=2020-12-23T00%3A59%3A00Z&sp=r\", \"logs/azureml/14332_azureml.log\": \"https://projektastorage1d32b8c8a.blob.core.windows.net/azureml/ExperimentRun/dcid.torchvision_1608655769_f46fb20e/logs/azureml/14332_azureml.log?sv=2019-02-02&sr=b&sig=fi9Mhj2ib%2BCnqhQoHevEBd2Yuhfnekgambnj8jOhaSI%3D&st=2020-12-22T16%3A39%3A39Z&se=2020-12-23T00%3A49%3A39Z&sp=r\", \"logs/azureml/dataprep/python_span_27eaf8b9-07df-4f14-a693-3b4ec44f21f2.jsonl\": \"https://projektastorage1d32b8c8a.blob.core.windows.net/azureml/ExperimentRun/dcid.torchvision_1608655769_f46fb20e/logs/azureml/dataprep/python_span_27eaf8b9-07df-4f14-a693-3b4ec44f21f2.jsonl?sv=2019-02-02&sr=b&sig=Q45hm%2BNq2nAy3sijshGsvS7pwCbIrp3NTd3WhE4f3%2FA%3D&st=2020-12-22T16%3A39%3A39Z&se=2020-12-23T00%3A49%3A39Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/dataprep/python_span_27eaf8b9-07df-4f14-a693-3b4ec44f21f2.jsonl\"], [\"azureml-logs/60_control_log.txt\"], [\"azureml-logs/70_driver_log.txt\"], [\"logs/azureml/14332_azureml.log\"]], \"run_duration\": \"0:09:17\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [{\"name\": \"mAP@IoU=0.50\", \"run_id\": \"torchvision_1608655769_f46fb20e\", \"categories\": [0], \"series\": [{\"data\": [0.9770205154885337]}]}], \"run_logs\": \"2020-12-22 16:49:35,152|azureml|DEBUG|Inputs:: kwargs: {'OutputCollection': True, 'EnableMLflowTracking': True, 'snapshotProject': False}, track_folders: None, deny_list: None, directories_to_watch: ['logs', 'logs/azureml']\\n2020-12-22 16:49:35,153|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Execution target type: none\\n2020-12-22 16:49:35,206|azureml.history._tracking.PythonWorkingDirectory|DEBUG|PySpark found in environment.\\n2020-12-22 16:49:35,206|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Pinning working directory for filesystems: ['pyfs']\\n2020-12-22 16:49:35,540|azureml.core._experiment_method|DEBUG|Trying to register submit_function search, on method <class 'azureml.train.hyperdrive.runconfig.HyperDriveRunConfig'>\\n2020-12-22 16:49:35,540|azureml.core._experiment_method|DEBUG|Registered submit_function search, on method <class 'azureml.train.hyperdrive.runconfig.HyperDriveRunConfig'>\\n2020-12-22 16:49:35,540|azureml.core._experiment_method|DEBUG|Trying to register submit_function search, on method <class 'azureml.train.hyperdrive.runconfig.HyperDriveConfig'>\\n2020-12-22 16:49:35,540|azureml.core._experiment_method|DEBUG|Registered submit_function search, on method <class 'azureml.train.hyperdrive.runconfig.HyperDriveConfig'>\\n2020-12-22 16:49:35,540|azureml.core.run|DEBUG|Adding new factory <function HyperDriveRun._from_run_dto at 0x7f6b3a014510> for run source hyperdrive\\n2020-12-22 16:49:36,385|azureml.core.run|DEBUG|Adding new factory <function AutoMLRun._from_run_dto at 0x7f6b1fd428c8> for run source automl\\n2020-12-22 16:49:36,396|azureml.core.run|DEBUG|Adding new factory <function PipelineRun._from_dto at 0x7f6b39d32d08> for run source azureml.PipelineRun\\n2020-12-22 16:49:36,404|azureml.core.run|DEBUG|Adding new factory <function StepRun._from_reused_dto at 0x7f6b39cbe7b8> for run source azureml.ReusedStepRun\\n2020-12-22 16:49:36,413|azureml.core.run|DEBUG|Adding new factory <function StepRun._from_dto at 0x7f6b39cbe730> for run source azureml.StepRun\\n2020-12-22 16:49:36,422|azureml.core.run|DEBUG|Adding new factory <function ScriptRun._from_run_dto at 0x7f6b3a1a7e18> for run source azureml.scriptrun\\n2020-12-22 16:49:36,452|azureml.core.authentication.TokenRefresherDaemon|DEBUG|Starting daemon and triggering first instance\\n2020-12-22 16:49:36,459|azureml._restclient.clientbase|INFO|Created a worker pool for first use\\n2020-12-22 16:49:36,459|azureml.core.authentication|DEBUG|Time to expire 1814393.540544 seconds\\n2020-12-22 16:49:36,459|azureml._restclient.service_context|DEBUG|Created a static thread pool for ServiceContext class\\n2020-12-22 16:49:36,459|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 16:49:36,460|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 16:49:36,460|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 16:49:36,460|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 16:49:36,460|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 16:49:36,460|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 16:49:36,460|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 16:49:36,489|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[START]\\n2020-12-22 16:49:36,489|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.RunClient|DEBUG|ClientBase: Calling get_by_exp_id with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experimentids/{experimentId}/runs/{runId}\\n2020-12-22 16:49:36,552|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[STOP]\\n2020-12-22 16:49:36,553|azureml._SubmittedRun#torchvision_1608655769_f46fb20e|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'local', 'ContentSnapshotId': None, 'azureml.git.repository_uri': 'https://github.com/microsoft/HyperdriveDeepLearning.git', 'mlflow.source.git.repoURL': 'https://github.com/microsoft/HyperdriveDeepLearning.git', 'azureml.git.branch': 'master', 'mlflow.source.git.branch': 'master', 'azureml.git.commit': '50577d753e96bc0b409215a08a544bce8bc7027b', 'mlflow.source.git.commit': '50577d753e96bc0b409215a08a544bce8bc7027b', 'azureml.git.dirty': 'True'}\\n2020-12-22 16:49:36,553|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2020-12-22 16:49:37,051|azureml|DEBUG|Installed with mlflow version 1.11.0.\\n2020-12-22 16:49:37,052|azureml.mlflow|DEBUG|Setting up a Remote MLflow run\\n2020-12-22 16:49:37,054|azureml.mlflow|DEBUG|Creating a tracking uri in eastus.experiments.azureml.net for workspace /subscriptions/1db0a5ce-7de1-4082-8e25-3c5a4e5a9a98/resourceGroups/ProjektAzure/providers/Microsoft.MachineLearningServices/workspaces/ProjektAzure\\n2020-12-22 16:49:37,054|azureml.mlflow._internal.store|DEBUG|Initializing the AzureMLRestStore\\n2020-12-22 16:49:37,054|azureml.mlflow._internal.model_registry|DEBUG|Initializing the AzureMLflowModelRegistry\\n2020-12-22 16:49:37,054|azureml.mlflow|DEBUG|Setting MLflow tracking uri env var\\n2020-12-22 16:49:37,054|azureml.mlflow|DEBUG|Setting MLflow run id env var with torchvision_1608655769_f46fb20e\\n2020-12-22 16:49:37,054|azureml.mlflow|DEBUG|Setting Mlflow experiment with torchvision\\n2020-12-22 16:49:37,055|azureml.mlflow|DEBUG|Setting the mlflow tag mlflow.source.type\\n2020-12-22 16:49:37,056|azureml.mlflow|DEBUG|Setting the mlflow tag mlflow.source.name\\n2020-12-22 16:49:37,056|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.RunClient.get_details-async:False|DEBUG|[START]\\n2020-12-22 16:49:37,056|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.RunClient|DEBUG|ClientBase: Calling get_details with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experiments/{experimentName}/runs/{runId}/details\\n2020-12-22 16:49:37,220|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.RunClient.get_details-async:False|DEBUG|[STOP]\\n2020-12-22 16:49:37,222|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.RunClient.patch_by_exp_id-async:False|DEBUG|[START]\\n2020-12-22 16:49:37,222|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.RunClient|DEBUG|ClientBase: Calling patch_by_exp_id with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experimentids/{experimentId}/runs/{runId}\\n2020-12-22 16:49:37,313|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.RunClient.patch_by_exp_id-async:False|DEBUG|[STOP]\\n2020-12-22 16:49:37,313|azureml.WorkerPool|DEBUG|[START]\\n2020-12-22 16:49:37,313|azureml.SendRunKillSignal|DEBUG|[START]\\n2020-12-22 16:49:37,313|azureml.RunStatusContext|DEBUG|[START]\\n2020-12-22 16:49:37,314|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunContextManager.RunStatusContext|DEBUG|[START]\\n2020-12-22 16:49:37,314|azureml.MetricsClient|DEBUG|[START]\\n2020-12-22 16:49:37,314|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.MetricsClient|DEBUG|[START]\\n2020-12-22 16:49:37,314|azureml.ContentUploader|DEBUG|[START]\\n2020-12-22 16:49:37,314|azureml._history.utils.context_managers|DEBUG|starting file watcher\\n2020-12-22 16:49:37,314|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|[Start]\\n2020-12-22 16:49:37,315|azureml.TrackFolders|DEBUG|[START]\\n2020-12-22 16:49:37,315|azureml.WorkingDirectoryCM|DEBUG|[START]\\n2020-12-22 16:49:37,315|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[START]\\n2020-12-22 16:49:37,315|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /tmp/azureml_runs/torchvision_1608655769_f46fb20e\\n2020-12-22 16:49:37,315|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2020-12-22 16:49:37,315|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Storing working dir for pyfs as /tmp/azureml_runs/torchvision_1608655769_f46fb20e\\n2020-12-22 16:49:37,321|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[START]\\n2020-12-22 16:49:37,321|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient|DEBUG|ClientBase: Calling batch_create_empty_artifacts with url /artifact/v2.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/artifacts/batch/metadata/{origin}/{container}\\n2020-12-22 16:49:37,556|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[STOP]\\n2020-12-22 16:49:38,085|azureml._history.utils.context_managers.FileWatcher|DEBUG|uploading data to container: azureml blob: ExperimentRun/dcid.torchvision_1608655769_f46fb20e/logs/azureml/14332_azureml.log path: /tmp/azureml_runs/torchvision_1608655769_f46fb20e/logs/azureml/14332_azureml.log\\n2020-12-22 16:49:38,113|azureml._history.utils.context_managers.FileWatcher|DEBUG|uploading data to container: azureml blob: ExperimentRun/dcid.torchvision_1608655769_f46fb20e/logs/azureml/dataprep/python_span_27eaf8b9-07df-4f14-a693-3b4ec44f21f2.jsonl path: /tmp/azureml_runs/torchvision_1608655769_f46fb20e/logs/azureml/dataprep/python_span_27eaf8b9-07df-4f14-a693-3b4ec44f21f2.jsonl\\n2020-12-22 16:49:38,113|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:49:38,114|azureml._history.utils.context_managers.FileWatcher.UploadQueue.0_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:49:38,114|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 0_result to queue of approximate size: 0\\n2020-12-22 16:50:06,454|azureml.core.authentication|DEBUG|Time to expire 1814363.546244 seconds\\n2020-12-22 16:50:08,118|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:50:08,119|azureml._history.utils.context_managers.FileWatcher.UploadQueue.1_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:50:08,119|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 1_result to queue of approximate size: 1\\n2020-12-22 16:50:36,453|azureml.core.authentication|DEBUG|Time to expire 1814333.546354 seconds\\n2020-12-22 16:50:38,121|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:50:38,121|azureml._history.utils.context_managers.FileWatcher.UploadQueue.2_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:50:38,122|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 2_result to queue of approximate size: 2\\n2020-12-22 16:51:06,453|azureml.core.authentication|DEBUG|Time to expire 1814303.546058 seconds\\n2020-12-22 16:51:08,123|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:51:08,124|azureml._history.utils.context_managers.FileWatcher.UploadQueue.3_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:51:08,124|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 3_result to queue of approximate size: 3\\n2020-12-22 16:51:36,454|azureml.core.authentication|DEBUG|Time to expire 1814273.545746 seconds\\n2020-12-22 16:51:38,125|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:51:38,126|azureml._history.utils.context_managers.FileWatcher.UploadQueue.4_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:51:38,126|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 4_result to queue of approximate size: 4\\n2020-12-22 16:52:06,454|azureml.core.authentication|DEBUG|Time to expire 1814243.545428 seconds\\n2020-12-22 16:52:08,127|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:52:08,127|azureml._history.utils.context_managers.FileWatcher.UploadQueue.5_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:52:08,128|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 5_result to queue of approximate size: 5\\n2020-12-22 16:52:36,454|azureml.core.authentication|DEBUG|Time to expire 1814213.545104 seconds\\n2020-12-22 16:52:38,129|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:52:38,130|azureml._history.utils.context_managers.FileWatcher.UploadQueue.6_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:52:38,130|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 6_result to queue of approximate size: 6\\n2020-12-22 16:53:06,455|azureml.core.authentication|DEBUG|Time to expire 1814183.544806 seconds\\n2020-12-22 16:53:08,131|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:53:08,132|azureml._history.utils.context_managers.FileWatcher.UploadQueue.7_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:53:08,132|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 7_result to queue of approximate size: 7\\n2020-12-22 16:53:36,455|azureml.core.authentication|DEBUG|Time to expire 1814153.544485 seconds\\n2020-12-22 16:53:38,133|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:53:38,134|azureml._history.utils.context_managers.FileWatcher.UploadQueue.8_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:53:38,134|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 8_result to queue of approximate size: 8\\n2020-12-22 16:54:06,455|azureml.core.authentication|DEBUG|Time to expire 1814123.544191 seconds\\n2020-12-22 16:54:08,135|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:54:08,136|azureml._history.utils.context_managers.FileWatcher.UploadQueue.9_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:54:08,136|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 9_result to queue of approximate size: 9\\n2020-12-22 16:54:18,139|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:54:18,139|azureml._history.utils.context_managers.FileWatcher.UploadQueue.10_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:54:18,139|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 10_result to queue of approximate size: 10\\n2020-12-22 16:54:36,456|azureml.core.authentication|DEBUG|Time to expire 1814093.543379 seconds\\n2020-12-22 16:54:38,141|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:54:38,141|azureml._history.utils.context_managers.FileWatcher.UploadQueue.11_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:54:38,141|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 11_result to queue of approximate size: 11\\n2020-12-22 16:55:06,456|azureml.core.authentication|DEBUG|Time to expire 1814063.543054 seconds\\n2020-12-22 16:55:08,142|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:55:08,143|azureml._history.utils.context_managers.FileWatcher.UploadQueue.12_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:55:08,143|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 12_result to queue of approximate size: 12\\n2020-12-22 16:55:36,457|azureml.core.authentication|DEBUG|Time to expire 1814033.542745 seconds\\n2020-12-22 16:55:38,145|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:55:38,145|azureml._history.utils.context_managers.FileWatcher.UploadQueue.13_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:55:38,147|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 13_result to queue of approximate size: 13\\n2020-12-22 16:55:48,148|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:55:48,149|azureml._history.utils.context_managers.FileWatcher.UploadQueue.14_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:55:48,149|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 14_result to queue of approximate size: 14\\n2020-12-22 16:55:58,152|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:55:58,152|azureml._history.utils.context_managers.FileWatcher.UploadQueue.15_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:55:58,152|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 15_result to queue of approximate size: 15\\n2020-12-22 16:56:06,457|azureml.core.authentication|DEBUG|Time to expire 1814003.542442 seconds\\n2020-12-22 16:56:08,153|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:56:08,153|azureml._history.utils.context_managers.FileWatcher.UploadQueue.16_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:56:08,154|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 16_result to queue of approximate size: 16\\n2020-12-22 16:56:36,457|azureml.core.authentication|DEBUG|Time to expire 1813973.542126 seconds\\n2020-12-22 16:56:38,155|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:56:38,155|azureml._history.utils.context_managers.FileWatcher.UploadQueue.17_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:56:38,155|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 17_result to queue of approximate size: 17\\n2020-12-22 16:57:06,458|azureml.core.authentication|DEBUG|Time to expire 1813943.541302 seconds\\n2020-12-22 16:57:08,157|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:57:08,157|azureml._history.utils.context_managers.FileWatcher.UploadQueue.18_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:57:08,157|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 18_result to queue of approximate size: 18\\n2020-12-22 16:57:36,459|azureml.core.authentication|DEBUG|Time to expire 1813913.540982 seconds\\n2020-12-22 16:57:38,163|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:57:38,164|azureml._history.utils.context_managers.FileWatcher.UploadQueue.19_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:57:38,164|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 19_result to queue of approximate size: 19\\n2020-12-22 16:58:06,459|azureml.core.authentication|DEBUG|Time to expire 1813883.540315 seconds\\n2020-12-22 16:58:08,166|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:58:08,167|azureml._history.utils.context_managers.FileWatcher.UploadQueue.20_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:58:08,167|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 20_result to queue of approximate size: 20\\n2020-12-22 16:58:36,459|azureml.core.authentication|DEBUG|Time to expire 1813853.54023 seconds\\n2020-12-22 16:58:38,168|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:58:38,169|azureml._history.utils.context_managers.FileWatcher.UploadQueue.21_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:58:38,169|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 21_result to queue of approximate size: 21\\n2020-12-22 16:58:41,369|azureml._restclient.service_context|DEBUG|Access an existing static threadpool for ServiceContext class\\n2020-12-22 16:58:41,369|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 16:58:41,370|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 16:58:41,370|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 16:58:41,370|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 16:58:41,370|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 16:58:41,370|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 16:58:41,371|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus.experiments.azureml.net.\\n2020-12-22 16:58:41,400|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[START]\\n2020-12-22 16:58:41,400|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.RunClient|DEBUG|ClientBase: Calling get_by_exp_id with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experimentids/{experimentId}/runs/{runId}\\n2020-12-22 16:58:41,471|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[STOP]\\n2020-12-22 16:58:41,472|azureml._SubmittedRun#torchvision_1608655769_f46fb20e|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'local', 'ContentSnapshotId': None, 'azureml.git.repository_uri': 'https://github.com/microsoft/HyperdriveDeepLearning.git', 'mlflow.source.git.repoURL': 'https://github.com/microsoft/HyperdriveDeepLearning.git', 'azureml.git.branch': 'master', 'mlflow.source.git.branch': 'master', 'azureml.git.commit': '50577d753e96bc0b409215a08a544bce8bc7027b', 'mlflow.source.git.commit': '50577d753e96bc0b409215a08a544bce8bc7027b', 'azureml.git.dirty': 'True'}\\n2020-12-22 16:58:41,472|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2020-12-22 16:58:41,472|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.MetricsClient|DEBUG|Overrides: Max batch size: 50, batch cushion: 5, Interval: 1.\\n2020-12-22 16:58:41,472|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.PostMetricsBatchV2Daemon|DEBUG|Starting daemon and triggering first instance\\n2020-12-22 16:58:41,472|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.MetricsClient|DEBUG|Used <class 'azureml._common.async_utils.batch_task_queue.BatchTaskQueue'> for use_batch=True.\\n2020-12-22 16:58:42,156|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2020-12-22 16:58:42,156|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /tmp/azureml_runs/torchvision_1608655769_f46fb20e\\n2020-12-22 16:58:42,156|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Reverting working dir from /tmp/azureml_runs/torchvision_1608655769_f46fb20e to /tmp/azureml_runs/torchvision_1608655769_f46fb20e\\n2020-12-22 16:58:42,156|azureml.history._tracking.PythonWorkingDirectory|INFO|Working dir is already updated /tmp/azureml_runs/torchvision_1608655769_f46fb20e\\n2020-12-22 16:58:42,156|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[STOP]\\n2020-12-22 16:58:42,156|azureml.WorkingDirectoryCM|DEBUG|[STOP]\\n2020-12-22 16:58:42,156|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Uploading tracked directories: ['./outputs'], excluding ['azureml-logs/driver_log']\\n2020-12-22 16:58:42,156|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling track for pyfs\\n2020-12-22 16:58:42,156|azureml.history._tracking.PythonWorkingDirectory|DEBUG|./outputs exists as directory, uploading..\\n2020-12-22 16:58:42,156|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Found and adding path to upload: ./outputs/model_latest.pth\\n2020-12-22 16:58:42,156|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Paths to upload is ['./outputs/model_latest.pth'] in dir ./outputs\\n2020-12-22 16:58:42,157|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|Overriding default timeout to 300\\n2020-12-22 16:58:42,157|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|[Start]\\n2020-12-22 16:58:42,157|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[START]\\n2020-12-22 16:58:42,157|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient|DEBUG|ClientBase: Calling batch_create_empty_artifacts with url /artifact/v2.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/artifacts/batch/metadata/{origin}/{container}\\n2020-12-22 16:58:42,290|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[STOP]\\n2020-12-22 16:58:42,290|azureml._restclient.service_context.WorkerPool|DEBUG|submitting future: perform_upload\\n2020-12-22 16:58:42,291|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient.upload_files.0_perform_upload|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:58:42,291|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|Adding task 0_perform_upload to queue of approximate size: 0\\n2020-12-22 16:58:42,291|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|[Stop] - waiting default timeout\\n2020-12-22 16:58:42,291|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient.upload_files.WaitFlushSource:upload_files|DEBUG|[START]\\n2020-12-22 16:58:42,291|azureml._restclient.clientbase|DEBUG|ClientBase: Calling create_blob_from_stream with url None\\n2020-12-22 16:58:42,291|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient.upload_files.WaitFlushSource:upload_files|DEBUG|Overriding default flush timeout from None to 300\\n2020-12-22 16:58:42,291|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient.upload_files.WaitFlushSource:upload_files|DEBUG|Waiting 300 seconds on tasks: [AsyncTask(0_perform_upload)].\\n2020-12-22 16:58:42,473|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|[Start]\\n2020-12-22 16:58:42,473|azureml.BatchTaskQueueAdd_1_Batches.WorkerPool|DEBUG|submitting future: _handle_batch\\n2020-12-22 16:58:42,473|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.MetricsClient.PostMetricsBatchV2|DEBUG|Batch size 1.\\n2020-12-22 16:58:42,474|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:58:42,474|azureml._restclient.service_context.WorkerPool|DEBUG|submitting future: _log_batch_v2\\n2020-12-22 16:58:42,474|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|Adding task 0__handle_batch to queue of approximate size: 0\\n2020-12-22 16:58:42,474|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.MetricsClient|DEBUG|Metrics Client: _log_batch_v2 is calling post_run_metrics posting 1 values.\\n2020-12-22 16:58:42,474|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.0__log_batch_v2|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:58:42,474|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|[Stop] - waiting default timeout\\n2020-12-22 16:58:42,474|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.MetricsClient.post_run_metrics-async:False|DEBUG|[START]\\n2020-12-22 16:58:42,474|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.MetricsClient.PostMetricsBatchV2|DEBUG|Adding task 0__log_batch_v2 to queue of approximate size: 0\\n2020-12-22 16:58:42,475|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|[START]\\n2020-12-22 16:58:42,475|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.MetricsClient|DEBUG|ClientBase: Calling post_run_metrics with url /metric/v2.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/runs/{runId}/batch\\n2020-12-22 16:58:42,475|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|Overriding default flush timeout from None to 120\\n2020-12-22 16:58:42,476|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|Waiting 120 seconds on tasks: [AsyncTask(0__handle_batch)].\\n2020-12-22 16:58:42,478|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:42,478|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch.WaitingTask|DEBUG|Awaiter is BatchTaskQueueAdd_1_Batches\\n2020-12-22 16:58:42,478|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:42,478|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|\\n2020-12-22 16:58:42,478|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|[STOP]\\n2020-12-22 16:58:42,596|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.MetricsClient.post_run_metrics-async:False|DEBUG|[STOP]\\n2020-12-22 16:58:44,554|azureml._file_utils.upload|DEBUG|Uploaded blob ExperimentRun/dcid.torchvision_1608655769_f46fb20e/outputs/model_latest.pth with size 176430266.\\n2020-12-22 16:58:44,795|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient.upload_files.0_perform_upload.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,795|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient.upload_files.0_perform_upload.WaitingTask|DEBUG|Awaiter is upload_files\\n2020-12-22 16:58:44,795|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient.upload_files.0_perform_upload.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,795|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|Waiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 8.869171142578125e-05 seconds.\\nWaiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 0.25039005279541016 seconds.\\nWaiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 0.5006921291351318 seconds.\\nWaiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 0.7510006427764893 seconds.\\nWaiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 1.0013155937194824 seconds.\\nWaiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 1.251636266708374 seconds.\\nWaiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 1.5019526481628418 seconds.\\nWaiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 1.7522706985473633 seconds.\\nWaiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 2.0025813579559326 seconds.\\nWaiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 2.2529001235961914 seconds.\\n\\n2020-12-22 16:58:44,795|azureml._SubmittedRun#torchvision_1608655769_f46fb20e.RunHistoryFacade.ArtifactsClient.upload_files.WaitFlushSource:upload_files|DEBUG|[STOP]\\n2020-12-22 16:58:44,795|azureml.TrackFolders|DEBUG|[STOP]\\n2020-12-22 16:58:44,795|azureml._history.utils.context_managers|DEBUG|exiting ContentUploader, waiting for file_watcher to finish upload...\\n2020-12-22 16:58:44,795|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher called finish, setting event\\n2020-12-22 16:58:44,795|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher received exit event, getting current_stat\\n2020-12-22 16:58:44,796|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:58:44,796|azureml._history.utils.context_managers.FileWatcher.UploadQueue.22_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:58:44,796|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 22_result to queue of approximate size: 22\\n2020-12-22 16:58:44,796|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher retrieved current_stat, will upload to current_stat\\n2020-12-22 16:58:44,796|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,799|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,799|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,799|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,800|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,800|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,800|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,800|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,800|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,801|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,801|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,801|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,801|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,801|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,802|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,802|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,802|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,802|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,803|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,803|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,803|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,803|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,803|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,804|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,804|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,804|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,804|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,805|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,805|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,805|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,805|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2020-12-22 16:58:44,806|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2020-12-22 16:58:44,806|azureml._history.utils.context_managers.FileWatcher.UploadQueue.23_result|DEBUG|Using basic handler - no exception handling\\n2020-12-22 16:58:44,807|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 23_result to queue of approximate size: 23\\n2020-12-22 16:58:44,807|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher finished uploading to current_stat, finishing task queue\\n2020-12-22 16:58:44,807|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|[Stop] - waiting default timeout\\n2020-12-22 16:58:44,807|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WaitFlushSource:UploadQueue|DEBUG|[START]\\n2020-12-22 16:58:44,809|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WaitFlushSource:UploadQueue|DEBUG|Overriding default flush timeout from None to 120\\n2020-12-22 16:58:44,809|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WaitFlushSource:UploadQueue|DEBUG|Waiting 120 seconds on tasks: [AsyncTask(0_result), AsyncTask(1_result), AsyncTask(2_result), AsyncTask(3_result), AsyncTask(4_result), AsyncTask(5_result), AsyncTask(6_result), AsyncTask(7_result), AsyncTask(8_result), AsyncTask(9_result), AsyncTask(10_result), AsyncTask(11_result), AsyncTask(12_result), AsyncTask(13_result), AsyncTask(14_result), AsyncTask(15_result), AsyncTask(16_result), AsyncTask(17_result), AsyncTask(18_result), AsyncTask(19_result), AsyncTask(20_result), AsyncTask(21_result), AsyncTask(22_result), AsyncTask(23_result)].\\n2020-12-22 16:58:44,809|azureml._history.utils.context_managers.FileWatcher.UploadQueue.0_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,809|azureml._history.utils.context_managers.FileWatcher.UploadQueue.0_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.0_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.1_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.1_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.1_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.2_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.2_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.2_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.3_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.3_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.3_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.4_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.4_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.4_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.5_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.5_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.5_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.6_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.6_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,810|azureml._history.utils.context_managers.FileWatcher.UploadQueue.6_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.7_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.7_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.7_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.8_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.8_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.8_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.9_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.9_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.9_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.10_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.10_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.10_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.11_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.11_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.11_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.12_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.12_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.12_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.13_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.13_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.13_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.14_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,811|azureml._history.utils.context_managers.FileWatcher.UploadQueue.14_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.14_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.15_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.15_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.15_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.16_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.16_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.16_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.17_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.17_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.17_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.18_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.18_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.18_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.19_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.19_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.19_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.20_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.20_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.20_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.21_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.21_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.21_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.22_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:44,812|azureml._history.utils.context_managers.FileWatcher.UploadQueue.22_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:44,813|azureml._history.utils.context_managers.FileWatcher.UploadQueue.22_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:45,063|azureml._history.utils.context_managers.FileWatcher.UploadQueue.23_result.WaitingTask|DEBUG|[START]\\n2020-12-22 16:58:45,063|azureml._history.utils.context_managers.FileWatcher.UploadQueue.23_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2020-12-22 16:58:45,063|azureml._history.utils.context_managers.FileWatcher.UploadQueue.23_result.WaitingTask|DEBUG|[STOP]\\n2020-12-22 16:58:45,063|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Waiting on task: 23_result.\\n1 tasks left. Current duration of flush 0.003776073455810547 seconds.\\n\\n2020-12-22 16:58:45,063|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WaitFlushSource:UploadQueue|DEBUG|[STOP]\\n\\nRun is completed.\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.17.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = exp.submit(estimator)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: torchvision_1608655769_f46fb20e\n",
      "Web View: https://ml.azure.com/experiments/torchvision/runs/torchvision_1608655769_f46fb20e?wsid=/subscriptions/1db0a5ce-7de1-4082-8e25-3c5a4e5a9a98/resourcegroups/ProjektAzure/workspaces/ProjektAzure\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "[2020-12-22T16:49:33.249848] Entering context manager injector.\n",
      "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['train.py', '--data_path', '.', '--workers', '8', '--learning_rate', '0.005', '--epochs', '1', '--anchor_sizes', '16,32,64,128,256,512', '--anchor_aspect_ratios', '0.25,0.5,1.0,2.0', '--rpn_nms_thresh', '0.5', '--box_nms_thresh', '0.3', '--box_score_thresh', '0.1'])\n",
      "Script type = None\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 14332\n",
      "Entering Run History Context Manager.\n",
      "[2020-12-22T16:49:37.538357] Current directory: /tmp/azureml_runs/torchvision_1608655769_f46fb20e\n",
      "[2020-12-22T16:49:37.538396] Preparing to call script [train.py] with arguments:['--data_path', '.', '--workers', '8', '--learning_rate', '0.005', '--epochs', '1', '--anchor_sizes', '16,32,64,128,256,512', '--anchor_aspect_ratios', '0.25,0.5,1.0,2.0', '--rpn_nms_thresh', '0.5', '--box_nms_thresh', '0.3', '--box_score_thresh', '0.1']\n",
      "[2020-12-22T16:49:37.538414] After variable expansion, calling script [train.py] with arguments:['--data_path', '.', '--workers', '8', '--learning_rate', '0.005', '--epochs', '1', '--anchor_sizes', '16,32,64,128,256,512', '--anchor_aspect_ratios', '0.25,0.5,1.0,2.0', '--rpn_nms_thresh', '0.5', '--box_nms_thresh', '0.3', '--box_score_thresh', '0.1']\n",
      "\n",
      "Epoch: [0]  [  0/279]  eta: 0:11:15  lr: 0.000023  loss: 1.6457 (1.6457)  loss_classifier: 0.8187 (0.8187)  loss_box_reg: 0.0472 (0.0472)  loss_objectness: 0.6917 (0.6917)  loss_rpn_box_reg: 0.0881 (0.0881)  time: 2.4197  data: 0.2689  max mem: 2922\n",
      "Epoch: [0]  [ 10/279]  eta: 0:07:56  lr: 0.000203  loss: 1.5104 (1.4345)  loss_classifier: 0.6241 (0.5432)  loss_box_reg: 0.0242 (0.0330)  loss_objectness: 0.6917 (0.6909)  loss_rpn_box_reg: 0.1587 (0.1674)  time: 1.7702  data: 0.0293  max mem: 4047\n",
      "Epoch: [0]  [ 20/279]  eta: 0:07:38  lr: 0.000382  loss: 1.1435 (1.2539)  loss_classifier: 0.1933 (0.3545)  loss_box_reg: 0.0289 (0.0516)  loss_objectness: 0.6899 (0.6890)  loss_rpn_box_reg: 0.1633 (0.1588)  time: 1.7382  data: 0.0045  max mem: 4611\n",
      "Epoch: [0]  [ 30/279]  eta: 0:07:24  lr: 0.000562  loss: 1.0595 (1.1989)  loss_classifier: 0.1447 (0.2997)  loss_box_reg: 0.0667 (0.0604)  loss_objectness: 0.6851 (0.6875)  loss_rpn_box_reg: 0.1546 (0.1512)  time: 1.7911  data: 0.0049  max mem: 4611\n",
      "Epoch: [0]  [ 40/279]  eta: 0:07:08  lr: 0.000742  loss: 1.0295 (1.1571)  loss_classifier: 0.1345 (0.2598)  loss_box_reg: 0.0689 (0.0714)  loss_objectness: 0.6801 (0.6846)  loss_rpn_box_reg: 0.1138 (0.1412)  time: 1.8153  data: 0.0061  max mem: 4611\n",
      "Epoch: [0]  [ 50/279]  eta: 0:06:48  lr: 0.000921  loss: 1.0271 (1.1461)  loss_classifier: 0.1133 (0.2361)  loss_box_reg: 0.1364 (0.0927)  loss_objectness: 0.6713 (0.6802)  loss_rpn_box_reg: 0.1069 (0.1371)  time: 1.7860  data: 0.0061  max mem: 4611\n",
      "Epoch: [0]  [ 60/279]  eta: 0:06:29  lr: 0.001101  loss: 1.0432 (1.1344)  loss_classifier: 0.1303 (0.2204)  loss_box_reg: 0.1630 (0.1116)  loss_objectness: 0.6457 (0.6714)  loss_rpn_box_reg: 0.1123 (0.1310)  time: 1.7536  data: 0.0064  max mem: 4611\n",
      "Epoch: [0]  [ 70/279]  eta: 0:06:10  lr: 0.001281  loss: 0.9296 (1.1053)  loss_classifier: 0.0978 (0.2023)  loss_box_reg: 0.1325 (0.1145)  loss_objectness: 0.6148 (0.6615)  loss_rpn_box_reg: 0.0886 (0.1270)  time: 1.7508  data: 0.0064  max mem: 4611\n",
      "Epoch: [0]  [ 80/279]  eta: 0:05:53  lr: 0.001460  loss: 0.9160 (1.0872)  loss_classifier: 0.0978 (0.1906)  loss_box_reg: 0.1422 (0.1282)  loss_objectness: 0.5468 (0.6444)  loss_rpn_box_reg: 0.0905 (0.1240)  time: 1.7727  data: 0.0062  max mem: 4611\n",
      "Epoch: [0]  [ 90/279]  eta: 0:05:34  lr: 0.001640  loss: 0.8644 (1.0646)  loss_classifier: 0.0770 (0.1773)  loss_box_reg: 0.1972 (0.1309)  loss_objectness: 0.4873 (0.6231)  loss_rpn_box_reg: 0.1218 (0.1332)  time: 1.7617  data: 0.0061  max mem: 4611\n",
      "Epoch: [0]  [100/279]  eta: 0:05:17  lr: 0.001820  loss: 0.8045 (1.0439)  loss_classifier: 0.0630 (0.1674)  loss_box_reg: 0.1434 (0.1358)  loss_objectness: 0.4409 (0.6062)  loss_rpn_box_reg: 0.1295 (0.1346)  time: 1.7480  data: 0.0062  max mem: 4611\n",
      "Epoch: [0]  [110/279]  eta: 0:05:00  lr: 0.001999  loss: 0.7735 (1.0186)  loss_classifier: 0.0710 (0.1576)  loss_box_reg: 0.1763 (0.1384)  loss_objectness: 0.4223 (0.5904)  loss_rpn_box_reg: 0.1111 (0.1321)  time: 1.8063  data: 0.0063  max mem: 4611\n",
      "Epoch: [0]  [120/279]  eta: 0:04:41  lr: 0.002179  loss: 0.6536 (0.9877)  loss_classifier: 0.0561 (0.1485)  loss_box_reg: 0.1274 (0.1377)  loss_objectness: 0.3710 (0.5676)  loss_rpn_box_reg: 0.1191 (0.1339)  time: 1.7535  data: 0.0063  max mem: 4611\n",
      "Epoch: [0]  [130/279]  eta: 0:04:23  lr: 0.002359  loss: 0.6423 (0.9658)  loss_classifier: 0.0592 (0.1431)  loss_box_reg: 0.1717 (0.1464)  loss_objectness: 0.2330 (0.5382)  loss_rpn_box_reg: 0.1601 (0.1380)  time: 1.7300  data: 0.0064  max mem: 4611\n",
      "Epoch: [0]  [140/279]  eta: 0:04:06  lr: 0.002538  loss: 0.5911 (0.9352)  loss_classifier: 0.0629 (0.1378)  loss_box_reg: 0.1882 (0.1485)  loss_objectness: 0.1539 (0.5098)  loss_rpn_box_reg: 0.1739 (0.1392)  time: 1.8009  data: 0.0062  max mem: 4611\n",
      "Epoch: [0]  [150/279]  eta: 0:03:49  lr: 0.002718  loss: 0.5046 (0.9042)  loss_classifier: 0.0614 (0.1331)  loss_box_reg: 0.1605 (0.1509)  loss_objectness: 0.1180 (0.4825)  loss_rpn_box_reg: 0.1299 (0.1378)  time: 1.8234  data: 0.0060  max mem: 4611\n",
      "Epoch: [0]  [160/279]  eta: 0:03:31  lr: 0.002898  loss: 0.4747 (0.8778)  loss_classifier: 0.0535 (0.1283)  loss_box_reg: 0.1551 (0.1524)  loss_objectness: 0.0859 (0.4589)  loss_rpn_box_reg: 0.1337 (0.1383)  time: 1.8073  data: 0.0061  max mem: 4611\n",
      "Epoch: [0]  [170/279]  eta: 0:03:13  lr: 0.003077  loss: 0.4740 (0.8501)  loss_classifier: 0.0518 (0.1245)  loss_box_reg: 0.1551 (0.1537)  loss_objectness: 0.0728 (0.4364)  loss_rpn_box_reg: 0.1056 (0.1355)  time: 1.7713  data: 0.0063  max mem: 4611\n",
      "Epoch: [0]  [180/279]  eta: 0:02:56  lr: 0.003257  loss: 0.3953 (0.8271)  loss_classifier: 0.0525 (0.1212)  loss_box_reg: 0.1974 (0.1554)  loss_objectness: 0.0775 (0.4163)  loss_rpn_box_reg: 0.0923 (0.1343)  time: 1.7826  data: 0.0065  max mem: 4611\n",
      "Epoch: [0]  [190/279]  eta: 0:02:37  lr: 0.003437  loss: 0.3903 (0.8061)  loss_classifier: 0.0525 (0.1184)  loss_box_reg: 0.1618 (0.1575)  loss_objectness: 0.0698 (0.3978)  loss_rpn_box_reg: 0.0963 (0.1323)  time: 1.7498  data: 0.0065  max mem: 4611\n",
      "Epoch: [0]  [200/279]  eta: 0:02:20  lr: 0.003616  loss: 0.4780 (0.7911)  loss_classifier: 0.0630 (0.1164)  loss_box_reg: 0.2103 (0.1616)  loss_objectness: 0.0574 (0.3815)  loss_rpn_box_reg: 0.0984 (0.1316)  time: 1.7337  data: 0.0061  max mem: 4611\n",
      "Epoch: [0]  [210/279]  eta: 0:02:02  lr: 0.003796  loss: 0.4780 (0.7768)  loss_classifier: 0.0630 (0.1148)  loss_box_reg: 0.2318 (0.1651)  loss_objectness: 0.0559 (0.3662)  loss_rpn_box_reg: 0.1032 (0.1307)  time: 1.7880  data: 0.0062  max mem: 4611\n",
      "Epoch: [0]  [220/279]  eta: 0:01:44  lr: 0.003976  loss: 0.4329 (0.7620)  loss_classifier: 0.0611 (0.1125)  loss_box_reg: 0.1900 (0.1675)  loss_objectness: 0.0636 (0.3531)  loss_rpn_box_reg: 0.1032 (0.1289)  time: 1.7753  data: 0.0062  max mem: 4611\n",
      "Epoch: [0]  [230/279]  eta: 0:01:26  lr: 0.004156  loss: 0.3984 (0.7457)  loss_classifier: 0.0569 (0.1103)  loss_box_reg: 0.1609 (0.1686)  loss_objectness: 0.0462 (0.3395)  loss_rpn_box_reg: 0.0818 (0.1272)  time: 1.7730  data: 0.0064  max mem: 4611\n",
      "Epoch: [0]  [240/279]  eta: 0:01:09  lr: 0.004335  loss: 0.3584 (0.7320)  loss_classifier: 0.0513 (0.1083)  loss_box_reg: 0.1383 (0.1682)  loss_objectness: 0.0426 (0.3279)  loss_rpn_box_reg: 0.1094 (0.1276)  time: 1.8024  data: 0.0065  max mem: 4611\n",
      "Epoch: [0]  [250/279]  eta: 0:00:51  lr: 0.004515  loss: 0.4191 (0.7188)  loss_classifier: 0.0591 (0.1065)  loss_box_reg: 0.1655 (0.1693)  loss_objectness: 0.0519 (0.3169)  loss_rpn_box_reg: 0.1083 (0.1262)  time: 1.8007  data: 0.0063  max mem: 4611\n",
      "Epoch: [0]  [260/279]  eta: 0:00:33  lr: 0.004695  loss: 0.3621 (0.7049)  loss_classifier: 0.0541 (0.1047)  loss_box_reg: 0.1756 (0.1686)  loss_objectness: 0.0479 (0.3067)  loss_rpn_box_reg: 0.0928 (0.1249)  time: 1.7990  data: 0.0064  max mem: 4611\n",
      "Epoch: [0]  [270/279]  eta: 0:00:15  lr: 0.004874  loss: 0.2945 (0.6899)  loss_classifier: 0.0458 (0.1023)  loss_box_reg: 0.1164 (0.1676)  loss_objectness: 0.0415 (0.2969)  loss_rpn_box_reg: 0.0783 (0.1231)  time: 1.7751  data: 0.0064  max mem: 4611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [278/279]  eta: 0:00:01  lr: 0.005000  loss: 0.2727 (0.6806)  loss_classifier: 0.0426 (0.1011)  loss_box_reg: 0.1063 (0.1676)  loss_objectness: 0.0379 (0.2894)  loss_rpn_box_reg: 0.0916 (0.1225)  time: 1.7316  data: 0.0062  max mem: 4611\n",
      "Epoch: [0] Total time: 0:08:15 (1.7745 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "<string>:6: DeprecationWarning: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n",
      "Test:  [ 0/50]  eta: 0:00:53  model_time: 0.8972 (0.8972)  evaluator_time: 0.0116 (0.0116)  time: 1.0759  data: 0.1642  max mem: 4611\n",
      "Test:  [49/50]  eta: 0:00:00  model_time: 0.8058 (0.7895)  evaluator_time: 0.0040 (0.0098)  time: 0.7834  data: 0.0061  max mem: 4611\n",
      "Test: Total time: 0:00:40 (0.8122 s / it)\n",
      "Averaged stats: model_time: 0.8058 (0.7895)  evaluator_time: 0.0040 (0.0098)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.551\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.977\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.566\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.329\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.556\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.585\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.077\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.470\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.628\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.327\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.636\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.632\n",
      "That's it!\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 14332\n",
      "\n",
      "\n",
      "[2020-12-22T16:58:41.948908] The experiment completed successfully. Finalizing run...\n",
      "[2020-12-22T16:58:41.948926] Start FinalizingInRunHistory\n",
      "[2020-12-22T16:58:41.949416] Logging experiment finalizing status in history service.\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.12182879447937012 seconds\n",
      "[2020-12-22T16:58:45.747162] Finished context manager injector.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: torchvision_1608655769_f46fb20e\n",
      "Web View: https://ml.azure.com/experiments/torchvision/runs/torchvision_1608655769_f46fb20e?wsid=/subscriptions/1db0a5ce-7de1-4082-8e25-3c5a4e5a9a98/resourcegroups/ProjektAzure/workspaces/ProjektAzure\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'torchvision_1608655769_f46fb20e',\n",
       " 'target': 'local',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2020-12-22T16:49:32.132115Z',\n",
       " 'endTimeUtc': '2020-12-22T16:58:48.558004Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'local',\n",
       "  'ContentSnapshotId': None,\n",
       "  'azureml.git.repository_uri': 'https://github.com/microsoft/HyperdriveDeepLearning.git',\n",
       "  'mlflow.source.git.repoURL': 'https://github.com/microsoft/HyperdriveDeepLearning.git',\n",
       "  'azureml.git.branch': 'master',\n",
       "  'mlflow.source.git.branch': 'master',\n",
       "  'azureml.git.commit': '50577d753e96bc0b409215a08a544bce8bc7027b',\n",
       "  'mlflow.source.git.commit': '50577d753e96bc0b409215a08a544bce8bc7027b',\n",
       "  'azureml.git.dirty': 'True'},\n",
       " 'inputDatasets': [],\n",
       " 'outputDatasets': [],\n",
       " 'runDefinition': {'script': 'train.py',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': ['--data_path',\n",
       "   '.',\n",
       "   '--workers',\n",
       "   '8',\n",
       "   '--learning_rate',\n",
       "   '0.005',\n",
       "   '--epochs',\n",
       "   '1',\n",
       "   '--anchor_sizes',\n",
       "   '16,32,64,128,256,512',\n",
       "   '--anchor_aspect_ratios',\n",
       "   '0.25,0.5,1.0,2.0',\n",
       "   '--rpn_nms_thresh',\n",
       "   '0.5',\n",
       "   '--box_nms_thresh',\n",
       "   '0.3',\n",
       "   '--box_score_thresh',\n",
       "   '0.1'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'local',\n",
       "  'dataReferences': {},\n",
       "  'data': {},\n",
       "  'outputData': {},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'priority': None,\n",
       "  'credentialPassthrough': False,\n",
       "  'environment': {'name': 'Experiment torchvision Environment',\n",
       "   'version': 'Autosave_2020-12-21T20:57:49Z_6cb444b5',\n",
       "   'python': {'interpreterPath': '/anaconda/envs/azureml_py36_pytorch/bin/python',\n",
       "    'userManagedDependencies': True,\n",
       "    'condaDependencies': {'channels': ['anaconda', 'conda-forge'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-defaults',\n",
       "        'torch==1.4.0',\n",
       "        'torchvision==0.5.0',\n",
       "        'horovod==0.18.1',\n",
       "        'tensorboard==1.14.0',\n",
       "        'future==0.17.1']}],\n",
       "     'name': 'project_environment'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE',\n",
       "    'NCCL_TREE_THRESHOLD': '0'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': False,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': False},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'enableMLflowTracking': True,\n",
       "   'snapshotProject': False},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'aiSuperComputer': {'instanceType': None,\n",
       "   'frameworkImage': None,\n",
       "   'imageVersion': None,\n",
       "   'location': None,\n",
       "   'aiSuperComputerStorageData': None},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': False,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'commandReturnCodeConfig': {'returnCode': 'Zero',\n",
       "   'successfulReturnCodes': []}},\n",
       " 'logFiles': {'azureml-logs/60_control_log.txt': 'https://projektastorage1d32b8c8a.blob.core.windows.net/azureml/ExperimentRun/dcid.torchvision_1608655769_f46fb20e/azureml-logs/60_control_log.txt?sv=2019-02-02&sr=b&sig=KCg7lEDWctWFWgGqoCmLZfhIrTIYHW5CyHNv01n2dPs%3D&st=2020-12-22T16%3A49%3A00Z&se=2020-12-23T00%3A59%3A00Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://projektastorage1d32b8c8a.blob.core.windows.net/azureml/ExperimentRun/dcid.torchvision_1608655769_f46fb20e/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=NyTNtcdlMEfaFxFE77VY2ITl7usTTfUmGuwAueSVg9Q%3D&st=2020-12-22T16%3A49%3A00Z&se=2020-12-23T00%3A59%3A00Z&sp=r',\n",
       "  'logs/azureml/14332_azureml.log': 'https://projektastorage1d32b8c8a.blob.core.windows.net/azureml/ExperimentRun/dcid.torchvision_1608655769_f46fb20e/logs/azureml/14332_azureml.log?sv=2019-02-02&sr=b&sig=fi9Mhj2ib%2BCnqhQoHevEBd2Yuhfnekgambnj8jOhaSI%3D&st=2020-12-22T16%3A39%3A39Z&se=2020-12-23T00%3A49%3A39Z&sp=r',\n",
       "  'logs/azureml/dataprep/python_span_27eaf8b9-07df-4f14-a693-3b4ec44f21f2.jsonl': 'https://projektastorage1d32b8c8a.blob.core.windows.net/azureml/ExperimentRun/dcid.torchvision_1608655769_f46fb20e/logs/azureml/dataprep/python_span_27eaf8b9-07df-4f14-a693-3b4ec44f21f2.jsonl?sv=2019-02-02&sr=b&sig=Q45hm%2BNq2nAy3sijshGsvS7pwCbIrp3NTd3WhE4f3%2FA%3D&st=2020-12-22T16%3A39%3A39Z&se=2020-12-23T00%3A49%3A39Z&sp=r'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azureml-logs/60_control_log.txt',\n",
       " 'azureml-logs/70_driver_log.txt',\n",
       " 'logs/azureml/14332_azureml.log',\n",
       " 'logs/azureml/dataprep/python_span_27eaf8b9-07df-4f14-a693-3b4ec44f21f2.jsonl',\n",
       " 'outputs/model_latest.pth']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mAP@IoU=0.50': 0.9770205154885337}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now register this first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='ProjektAzure', subscription_id='1db0a5ce-7de1-4082-8e25-3c5a4e5a9a98', resource_group='ProjektAzure'), name=torchvision_local_model, id=torchvision_local_model:1, version=1, tags={}, properties={})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.register_model(model_name=\"torchvision_local_model\", model_path=\"/outputs/model_latest.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download our model and load it to make predictions on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file(\"outputs/model_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "anchor_sizes = \"16,32,64,128,256,512\"\n",
    "anchor_aspect_ratios = \"0.25,0.5,1.0,2.0\"\n",
    "rpn_nms_threshold = 0.5\n",
    "box_nms_threshold = 0.3\n",
    "box_score_threshold = 0.1\n",
    "num_box_detections = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Mask RCNN model\n",
    "model = get_model(\n",
    "    num_classes,\n",
    "    anchor_sizes,\n",
    "    anchor_aspect_ratios,\n",
    "    rpn_nms_threshold,\n",
    "    box_nms_threshold,\n",
    "    box_score_threshold,\n",
    "    num_box_detections,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign()\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "    (mask_roi_pool): None\n",
       "    (mask_head): MaskRCNNHeads(\n",
       "      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu2): ReLU(inplace=True)\n",
       "      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu3): ReLU(inplace=True)\n",
       "      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu4): ReLU(inplace=True)\n",
       "    )\n",
       "    (mask_predictor): MaskRCNNPredictor(\n",
       "      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask_fcn_logits): Conv2d(256, 91, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"model_latest.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a random subset of the data to visualize predictions on the images.\n",
    "data_path = \"./scripts\"\n",
    "dataset = BuildDataset(data_path, get_transform(train=False))\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(dataset)):\n",
    "#     img, _ = dataset[i]\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         prediction = model([img.to(device)])\n",
    "#     img = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
    "#     preds = prediction[0][\"boxes\"].cpu().numpy()\n",
    "#     print(prediction[0][\"scores\"])\n",
    "#     draw = ImageDraw.Draw(img)\n",
    "#     for i in range(len(preds)):\n",
    "#         draw.rectangle(\n",
    "#             ((preds[i][0], preds[i][1]), (preds[i][2], preds[i][3])), outline=\"red\"\n",
    "#         )\n",
    "#     display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook, we  will [build a custom docker image and push it to Azure Container Registry](03_BuildDockerImage.ipynb). This image will be used for tunning the hyperparameters of the model on AzureMLCompute."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "maxluk"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "msauthor": "minxia"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
